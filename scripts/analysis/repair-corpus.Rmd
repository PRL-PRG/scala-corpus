---
title: "Repair corpus"
output: html_document
editor_options: 
  chunk_output_type: console
---

This just constains snippets used to correct mistakes in running some tasks.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)

library(fs)
library(tidyverse)
library(ggplot2)
library(DT)

theme_set(theme_minimal())
source("inc/paths.R")
source("inc/stats.R")
```

## Load

```{r}
GH <- "/var/lib/scala/corpora/github/" 
SD <- "/var/lib/scala/corpora/scaladex/" 
```

## Check github and scaladex

### metadata

```{r}
gh_metadata_status <- read_csv(path(GH, "metadata-status.csv")) %>% mutate(project_id=tolower(project_id))
sd_metadata_status <- read_csv(path(SD, "metadata-status.csv")) %>% mutate(project_id=tolower(project_id))
interset_metadata_status <- inner_join(gh_metadata_status, sd_metadata_status, by="project_id", suffix=c("_gh", "_sd"))
```

```{r}
filter(interset_metadata_status, exit_code_gh == 1 & exit_code_sd == 0)
```

### compile

```{r}
gh_compile_status <- read_csv(path(GH, "compile-status.csv")) %>% mutate(project_id=tolower(project_id))
sd_compile_status <- read_csv(path(SD, "compile-status.csv")) %>% mutate(project_id=tolower(project_id))
interset_compile_status <- inner_join(gh_compile_status, sd_compile_status, by="project_id", suffix=c("_gh", "_sd"))
```

```{r}
filter(interset_compile_status, exit_code_gh == 1 & exit_code_sd == 0)
```

```{r}
cat(str_c(filter(interset_compile_status, exit_code_gh == 1 & exit_code_sd == 0)$project_id),sep="\n")
```


### semanticdb

```{r}
gh_semanticdb_status <- read_csv(path(GH, "semanticdb-status.csv")) %>% mutate(project_id=tolower(project_id))
sd_semanticdb_status <- read_csv(path(SD, "semanticdb-status.csv")) %>% mutate(project_id=tolower(project_id))
interset_semanticdb_status <- inner_join(gh_semanticdb_status, sd_semanticdb_status, by="project_id", suffix=c("_gh", "_sd"))
```

```{r}
anti_join(sd_semanticdb_status, gh_semanticdb_status, by="project_id")
```

```{r}
filter(interset_semanticdb_status, exit_code_gh == 1 & exit_code_sd == 0)$project_id
```

```{r}
gh_semanticdb_stats <- read_csv(path(GH, "semanticdb-stats.csv")) %>% mutate(project_id=tolower(project_id))
sd_semanticdb_stats <- read_csv(path(SD, "semanticdb-stats.csv")) %>% mutate(project_id=tolower(project_id))
interset_semanticdb_stats <- inner_join(gh_semanticdb_stats, sd_semanticdb_stats, by="project_id", suffix=c("_gh", "_sd"))
```

```{r}
semanticdb_success <- filter(interset_semanticdb_status, exit_code_gh == 0, exit_code_sd == 0)
interset_semanticdb_stats_success <- semi_join(interset_semanticdb_stats, semanticdb_success, by="project_id")
filter(interset_semanticdb_stats_success, files_gh != files_sd)
```

```{r}
gh_df <- read_csv(path(GH, "semanticdb-status.csv")) %>% mutate(project_id_norm=tolower(project_id))
gh_df_redo <- semi_join(gh_df, tibble(project_id_norm=done), by="project_id_norm")

sd_df <- read_csv(path(SD, "semanticdb-status.csv")) %>% mutate(project_id_norm=tolower(project_id))
sd_df_redo <- semi_join(sd_df, tibble(project_id_norm=done), by="project_id_norm")

projects_to_remove <- path(GH, "all-projects", gh_df_redo$project_id)
all(dir_exists(projects_to_remove))

projects_to_copy <- path(SD, "all-projects", sd_df_redo$project_id)
all(dir_exists(projects_to_copy))

length(projects_to_remove)
length(projects_to_copy)

dir_delete(projects_to_remove)
dir_copy(projects_to_copy, path(GH, "all-projects", basename(projects_to_copy)))
```

```{r}
file_exists(path(GH, "all-projects",sd_df_redo$project_id))
```


## Remove duplicates from all-projects.txt

```{r}
writeLines(unique(all_projects_names), ALL_PROJECTS_FILE)
```

## Sync pinned github info

```{r}
github_info <- read_csv(PROJECTS_GH_INFO_PINNED) %>% distinct(project_id, .keep_all = T)
semi_join(github_info, repo_metadata_sbt_compatible, by="project_id") %>% write_csv(PROJECTS_GH_INFO_PINNED)
```


## Remove empty repo-metadata.csv / repo-sloc.csv

```{r}
missing <- setdiff(downloaded, repo_metadata$project_id)
missing_df <- map_dfr(missing, ~tibble(project_id=., path=path(ALL_PROJECTS_DIR, .), files=if (dir_exists(path)) dir_ls(path, recursive = F) else character(0)))
repo_metadata_files <- path(unique(missing_df$path), "_analysis_", "repo-metadata.csv")
repo_sloc_files <- path(unique(missing_df$path), "_analysis_", "repo-sloc.csv")
file_delete(repo_metadata_files[file_exists(repo_metadata_files)])
file_delete(repo_sloc_files[file_exists(repo_sloc_files)])
```

```{r}
to_remove <- count(missing_df, path) %>% filter(n != 4) %>% mutate(
  repo_metadata_file=path(path, "_analysis_", "repo-metadata.csv"),
  repo_sloc_file=path(path, "_analysis_", "repo-metadata.csv")
)
file_delete(to_remove$repo_metadata_file[file_exists(to_remove$repo_metadata_file)])
file_delete(to_remove$repo_sloc_file[file_exists(to_remove$repo_sloc_file)])
```

## Remove repo-metadata when sbt_version is NA

```{r}
withr::with_dir("/var/lib/scala/corpora/github", {
  repo_metadata <- read_csv("repo-metadata.csv")
  repo_metadata %>% 
    filter(build_system=="sbt", is.na(sbt_version)) %>%
    transmute(path=path("all-projects", project_id, "_analysis_", "repo-metadata.csv")) %>%
    filter(file_exists(path)) %>%
    .$path %>% file_delete()
})
```

## Remove status file in projects that can redo

```{r}

repeatable_projects <- filter(failed_projects, cause %in% c("out-of-memory", "unknown-metadata-command", "unknown", "java-error"))$project_id
writeLines(repeatable_projects, path(params$base_dir, "redo.txt"))
```


## Fix the curpus merge

```{r}
github_all_projects <- read_lines("/var/lib/scala/corpora/github/all-projects.txt")
github_all_projects_norm <- tolower(github_all_projects)
github_all_projects_dir <- basename(dir_ls("/var/lib/scala/corpora/github/all-projects"))
scaladex_all_projects <- read_lines("/var/lib/scala/corpora/scaladex/all-projects.txt")
scaladex_all_projects_norm <- tolower(scaladex_all_projects)
scaladex_all_projects_dir <- basename(dir_ls("/var/lib/scala/corpora/scaladex/all-projects"))
```

```{r}
setdiff(c(1,2,3), c(2,3,4))
```

### Fix missing projects from scaladex to github

```{r}
missing_in_github <- setdiff(scaladex_all_projects, github_all_projects)
github_df <- tibble(project_id=tolower(github_all_projects), dir=github_all_projects)
scaladex_df <- tibble(project_id=tolower(scaladex_all_projects), dir=scaladex_all_projects)

stopifnot(all(dir_exists(path("/var/lib/scala/corpora/github/all-projects/", github_df$dir))))
stopifnot(all(dir_exists(path("/var/lib/scala/corpora/scaladex/all-projects/", scaladex_df$dir))))

missing_df <- anti_join(scaladex_df, github_df, by="project_id")
missing_projects <- missing_df$project_id

pblapply(missing_projects[-1], function(x) dir_copy(
  path("/var/lib/scala/corpora/scaladex/all-projects/",x),
  path("/var/lib/scala/corpora/github/all-projects/",x)
))
```

```{r}
new_github_all_projects <- c(github_all_projects, missing_projects)
github_all_projects_dir <- basename(dir_ls("/var/lib/scala/corpora/github/all-projects"))
stopifnot(setequal(new_github_all_projects, github_all_projects_dir))
writeLines(new_github_all_projects, "/var/lib/scala/corpora/github/all-projects.txt")
```

##@ Fix github info

```{r}
github_sbt_projects <- read_lines("/var/lib/scala/corpora/github/sbt-projects.txt")
github_gh <- bind_rows(
  read_csv("/var/lib/scala/corpora/github/projects-github-info.csv.pinned"),
  read_csv("/var/lib/scala/corpora/scaladex/projects-github-info.csv.pinned"),
  read_csv("/var/lib/scala/corpora/info.csv")
) %>% distinct(project_id, .keep_all = T)

github_gh_keep <- semi_join(
  github_gh,
  tibble(project_id=github_sbt_projects),
  by="project_id"
)

write_csv(github_gh_keep, "/var/lib/scala/corpora/github/projects-github-info.csv.pinned")

missing_gh <- setdiff(github_sbt_projects, github_gh$project_id)
writeLines(missing_gh, "/var/lib/scala/corpora/gh.txt")
```

```{r}
github_sbt_projects <- read_lines("/var/lib/scala/corpora/github/sbt-projects.txt")
info_new <- readr::read_csv("/var/lib/scala/corpora/info.csv")
info_old <- readr::read_csv("/var/lib/scala/corpora/github/projects-github-info.csv")
info_scaladex <- read_csv("/var/lib/scala/corpora/scaladex/projects-github-info.csv")

info <- bind_rows(
 info_new,
 info_old,
 info_scaladex
) %>% 
  filter(is.na(error)) %>%
  mutate(
    name=tolower(name),
    project_id_norm=tolower(project_id),
    corpus_name=sapply(strsplit(name, "/"), paste0, collapse="--")
  )

same_names <- filter(info, project_id_norm==corpus_name) %>% distinct(name, .keep_all = T)
sum(duplicated(same_names$project_id))

others <- anti_join(info, same_names, by="name") %>% distinct(name, .keep_all = T)
sum(duplicated(others$project_id))

final <- bind_rows(same_names, others)

sum(duplicated(final$name))
sum(duplicated(final$project_id))
final[which(duplicated(final$project_id)),]

final <- final %>% distinct(project_id, .keep_all = T)

sbt_df <- tibble(sbt_project_id=github_sbt_projects, project_id_norm=tolower(sbt_project_id))
final2 <- left_join(final, sbt_df, by="project_id_norm") %>% mutate(project_id=if_else(!is.na(sbt_project_id), sbt_project_id, project_id))

x <- mutate(final2, p=path("/var/lib/scala/corpora/github/all-projects", project_id), e=dir_exists(p))
filter(x, !e)

write_csv(select(final2, -corpus_name, -project_id_norm, -sbt_project_id), "/var/lib/scala/corpora/github/projects-github-info.csv.pinned")
write_csv(select(final2, -corpus_name, -project_id_norm, -sbt_project_id), "/var/lib/scala/corpora/github/projects-github-info.csv")
```

```{r}
github_dir <- basename(dir_ls("/var/lib/scala/corpora/github/all-projects"))
scaladex_dir <- basename(dir_ls("/var/lib/scala/corpora/scaladex/projects"))
setdiff(tolower(scaladex_dir), tolower(github_dir))
```

```{r}
github_projects <- read_lines("/var/lib/scala/corpora/github/all-projects.txt")
scaladex_projects <- read_lines("/var/lib/scala/corpora/scaladex/projects.txt")
setdiff(tolower(scaladex_projects), tolower(github_projects))
```

```{r}
paths <- path("/var/lib/scala/corpora/github/all-projects", read_lines("/var/lib/scala/corpora/github/all-projects.txt"))
all(dir_exists(paths))
```


### Check

```{r}
github_sbt_projects <- read_lines("/var/lib/scala/corpora/github/sbt-projects.txt")
github_so_projects <- read_lines("/var/lib/scala/corpora/github/so-projects.txt")
scaladex_so_projects <- read_lines("/var/lib/scala/corpora/scaladex/so-projects.txt")

gh <- tibble(p=github_so_projects, pl=tolower(p))
sd <- tibble(p=scaladex_so_projects, pl=tolower(p))

anti_join(sd, gh, by="pl")

setdiff(tolower(scaladex_so_projects), tolower(github_so_projects))
setdiff(tolower(scaladex_so_projects), tolower(filter(info_scaladex, is.na(error))$project_id))
setdiff(tolower(scaladex_so_projects), tolower(final$project_id))
```


### Fix the difference between all-projects.txt all-projects in scaladex

```{r}
stopifnot(all(!duplicated(scaladex_all_projects_dir)))
```

```{r}
missing_in_scaladex_projects <- setdiff(scaladex_all_projects_dir, scaladex_all_projects)
new_scaladex_projects <- c(scaladex_all_projects, missing_in_scaladex_projects)
setequal(new_scaladex_projects, scaladex_all_projects_dir)
write_lines(new_scaladex_projects, "/var/lib/scala/corpora/scaladex/all-projects.txt")
```

### Fix the difference between all-projects.txt all-projects in github

```{r}
stopifnot(all(!duplicated(github_all_projects_dir)))
```

```{r}
duplicates_in_github <- github_all_projects[duplicated(github_all_projects)]
length(github_all_projects) - length(duplicates_in_github) == length(github_all_projects_dir)
```
```{r}
length(intersect(duplicates_in_github, github_all_projects_dir)) == length(duplicates_in_github)
```
```{r}
new_github_all_projects <- c(setdiff(github_all_projects, duplicates_in_github), duplicates_in_github)

length(new_github_all_projects)
length(new_github_all_projects) == length(github_all_projects_dir)

length(intersect(new_github_all_projects, github_all_projects_dir)) == length(github_all_projects_dir)
setequal(new_github_all_projects, github_all_projects_dir)
```
```{r}
writeLines(new_github_all_projects, "/var/lib/scala/corpora/github/all-projects.txt")
```

## fix missing semanticbd

```{r}
metadata <- read_csv("/var/lib/scala/corpora/scaladex/metadata-status.csv")
semanticdb <- read_csv("/var/lib/scala/corpora/scaladex/semanticdb-status.csv")
semanticdb_good <- filter(semanticdb, exit_code==0)
bins <- select(semanticdb_good, project_id) %>% mutate(bin=path("/var/lib/scala/corpora/scaladex/projects", project_id, "_analysis_", "semanticdb.bin"), bin_exists=file_exists(bin), bin_size=file.size(bin))
missing <- filter(bins, !bin_exists | bin_size==0)

files <- path("/var/lib/scala/corpora/scaladex/projects", missing$project_id, "_analysis_", "semanticdb-stats.csv")
files <- files[file_exists(files)]
head(files)
file_delete(files)
```

## fix missing sbt_version

```{r}
x <- read_csv("/var/lib/scala/corpora/scaladex/repo-metadata.csv")
y <- filter(x, is.na(sbt_version)) %>% .$project_id %>% path("/var/lib/scala/corpora/scaladex/all-projects", ., "_analysis_", "repo-metadata.csv")
y <- y[file_exists(y)]
length(y)
head(y)
file_delete(y)
```

## sbt_version

```{r}
count(repo, sbt_version) %>% print(n=Inf)
```


## redo stuff in github

```{r}
info <- read_csv("/var/lib/scala/corpora/github/projects-github-info.csv")
repo <- read_csv("/var/lib/scala/corpora/github/repo-metadata.csv")
implicits <- read_csv("/var/lib/scala/corpora/github/implicits-stats.csv")
compile <- read_csv("/var/lib/scala/corpora/github/compile-status.csv") 
metadata <- read_csv("/var/lib/scala/corpora/github/metadata-status.csv") 
semanticdb <- read_csv("/var/lib/scala/corpora/github/semanticdb-status.csv") 
semanticdb_stats <- read_csv("/var/lib/scala/corpora/github/semanticdb-stats.csv") 

count(compile, exit_code)
count(metadata, exit_code)
count(semanticdb, exit_code)

filter(metadata, exit_code > 1)

compile_errors <- withr::with_dir("/var/lib/scala/corpora/github", phase_failure_cause_from_status(compile, "compile.log"))
metadata_errors <- withr::with_dir("/var/lib/scala/corpora/github", phase_failure_cause_from_status(metadata, "metadata.log"))
semanticdb_errors <- withr::with_dir("/var/lib/scala/corpora/github", phase_failure_cause_from_status(semanticdb, "semanticdb.log"))

count(compile_errors, exit_code)
count(metadata_errors, cause)
count(semanticdb_errors, cause)

bind_rows(
  select(filter(compile, exit_code==130), project_id),
  select(filter(compile_errors, cause=="out-of-memory"), project_id)
) %>% 
  distinct(project_id) %>%
  mutate(p=path("/var/lib/scala/corpora/github/all-projects/", project_id, "_analysis_/compile.log")) %>%
  filter(file_exists(p)) %>%
  .$p %>%
  file_delete()

bind_rows(
  select(filter(semanticdb, exit_code==130), project_id),
  select(filter(semanticdb_errors, cause=="out-of-memory"), project_id)
) %>% 
  distinct(project_id) %>%
  mutate(p=path("/var/lib/scala/corpora/github/all-projects/", project_id, "_analysis_/semanticdb-status.csv")) %>%
  filter(file_exists(p)) %>%
  .$p %>%
  file_delete()


  select(filter(metadata, exit_code==130), project_id)
  select(filter(semanticdb, exit_code==130), project_id)
  select(filter(metadata_errors, cause=="out-of-memory"), project_id)
  select(filter(semanticdb_errors, cause=="out-of-memory"), project_id)
```

```{r}
so_projects <- tibble(project_id=read_lines("/var/lib/scala/corpora/github/so-projects.txt"))
repo <- read_csv("/var/lib/scala/corpora/github/repo-metadata.csv")
github <- read_csv("/var/lib/scala/corpora/github/projects-github-info.csv")
info <- left_join(github, repo, by="project_id") %>% semi_join(so_projects)
dejavu_project_duplication <- read_csv("/var/lib/scala/corpora/github/dejavu-duplication.csv") %>% semi_join(so_projects)
dejavu_files_hash <- read_csv("/var/lib/scala/corpora/github/dejavu-files-hash-h2i.csv") %>% semi_join(so_projects)
```

```{r}
full_duplicates <- filter(dejavu_project_duplication, duplication==1)
info_dups <- semi_join(info, full_duplicates, by="project_id")
info_dups
```


```{r}
dejavu_project_duplication %>%
  filter(duplication > 0) %>%
  ggplot(aes(duplication)) +
  geom_histogram(bins=100) +
  scale_x_continuous(labels=scales::percent) +
  scale_y_continuous(labels=scales::comma) +
  labs(title="Dejavu duplication", subtitle="Number of duplicated files / number of files", x="Duplication", y="Number of projects")
```

```{r}
repo_metadata <- repo_metadata <- read_csv("/var/lib/scala/corpora/scaladex/repo-metadata.csv", col_types=cols(
  project_id = col_character(),
  build_system = col_character(),
  sbt_version = col_character(),
  size = col_double(),
  commit_count = col_double(),
  commit = col_character(),
  commit_date = col_double(),
  first_commit_date = col_double(),
  scala_code = col_double(),
  scala_files = col_double()
))

dejavu_project_duplication <- read_csv("/var/lib/scala/corpora/scaladex/dejavu-duplication.csv")
  

filter(dejavu_project_duplication, duplication==1) %>% print(n=Inf)
```

```{r}
find_full_duplicates <- function(project_name) {
  project <- filter(dejavu_files_hash, project_id==project_name)
  shared_files <- semi_join(dejavu_files_hash, project, by="hash")
  projects <- count(shared_files, project_id)
  
  left_join(
    projects,
    dejavu_project_duplication,
    by="project_id"
  ) %>% 
    filter(duplication==1, n==n_files, project_id!=project_name) %>%
    rename(duplicate=project_id) %>%
    transmute(project_id=project_name, duplicate)
}
```

```{r}
full_duplicates <- left_join(left_join(filter(dejavu_project_duplication, duplication==1), repo_metadata, by="project_id"), info, by="project_id")
single <- full_duplicates %>% semi_join(filter(count(full_duplicates, scala_code), n==1), by="scala_code")

x <- map_dfr(single$project_id, ~find_full_duplicates(.))
```

```{r}
so_projects <- tibble(project_id=read_lines())
dejavu_project_duplication <- read_lines() %>% semi_join(so_projects, by="project_id")
github_info <- read_lines() %>% semi_join(so_projects, by="project_id")
repo_metadata <- read_lines() %>% semi_join(so_projects, by="project_id")

duplicates <- filter(dejavu_project_duplication, duplication==1)
duplicates_info <- left_join(github_info, repo_metadata, by="project_id") %>% semi_join(duplicates, by="project_id")
to_keep <- filter(duplicates_info, scala_code >= 100, stars >= 2) %>% 
  group_by(scala_code) %>% 
  top_n(1, -updated_at) %>%
  upgroup() %>%
  .$project_id
```

