---
title: "Stage 3 Analysis"
authors: Filip Krikava, Jan Vitek and Heather Miller
output:
  html_document:
    code_folding: hide
    theme: united
    toc: true
    toc_float: true
params:
  corpus_dir: /var/lib/scala/corpora/github
  corpus_url: http://prl1.ele.fit.cvut.cz:8149
  lib_dir: ../inc
  report_name: corpus-analysis
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source(file.path(params$lib_dir, "setup.R"))
```

```{r output, include=FALSE}
OUT_CORPUS_BUILDING_TEX <- path(output_dir, "corpus-building.tex")
OUT_CORPUS_OVERVIEW_PDF <- path(output_dir, "corpus-overview.pdf")
OUT_SCALA_VERSIONS_TEX <- path(output_dir, "scala-version.tex")
OUT_TOP_PROJECTS_TEX <- path(output_dir, "top-projects.tex")
OUT_CATEGORIES_TEX <- path(output_dir, "categories.tex")
```

## Overview

The pipeline runs in three stages:

1. stage - downloads all the projects and gathers the repository-level metadata
2. stage - extracts sbt project metadata, compiles and generates semanticdb
3. stage - extracts implicits and put all the information together

The ``r CORPUS_STAGE1`` contains information about projects that finished stage 1 and indicates which can go to stage 2 before de-duplication.
The ``r CORPUS_STAGE3`` contains information about projects that went to stage 2 and indicates which managed to pass stage 3.
So `stage1_corpus` contains all Scala projects we could get, `stage2_corpus` contains all compatible de-duplicated projects and `corpus` is the final corpus containing only the projects we could extract implicits from.
The ``r CLEAN_CORPUS`` contains informaton about the projects after data cleaning. This is the final corpus.

The ``r CLEAN_CORPUS`` file contains the following columns:

- `project_id` (chr): project name as `github-user-name--github-repository-name`
- `origin` (chr): the URL to github 
- `build_system` (chr): guessed build system
- `sbt_version` (chr):  guessed sbt version
- `size_repo` (int): of the git versioned files in bytes
- `size` (int): of everything in bytes
- `commit_count` (int): number of commits
- `commit` (chr): current checkout hash
- `commit_date` (dttm): the date of the current commit
- `first_commit_date` (dttm): the date of the first commit
- `scala_code` (int): number of lines of Scala code excluding blanks and comments
- `scala_files` (int): number of Scala files
- `dejavu_n_files` (int): number of files processed by Dejavu (this shall be less than `scala_files` since it only consider non-token-empty ones with `.scala suffix`)
- `dejavu_n_duplicated_files` (int):  number of duplicate files (files seen in other projects)
- `dejavu_duplication` (dbl): `dejavu_n_duplicated_files/dejavu_n_files`
- `gh_name` (chr): GitHub name
- `gh_stars` (int): Number of stars 
- `gh_watchers` (int): Number of watchers
- `gh_created_at` (dttm): When was the project created at GitHub
- `gh_updated_at` (dttm): When was the project last updated at GitHub
- `gh_pushed_at` (dttm): When it has been last pushed to the project at GitHub
- `gh_fork` (lgl): Is it a fork?
- `gh_archived` (lgl): Has it been archived?
- `scaladex` (lgl): Has it been indexed by scaladex?
- `metadata_exit_code` (int): exit code of running `sbt metadata`
- `metadata_duration` (int): how long did it take (in seconds)
- `metadata_scala_code_test_managed` (int): managed Scala code in test
- `metadata_scala_code_test` (int): unmanaged and managed Scala code in test
- `metadata_scala_code_compile_managed` (int): managed Scala code in compile
- `metadata_scala_code_compile` (int): unmanaged and managed Scala code in compile
- `metadata_scala_code` (int): all Scala code above
- `modules` (int): number of subprojects
- `scala_version` (chr): scala version `sbt show scalaVersion` 
- `updated_scala_version` (chr): the scala version we used for semanticdb (some projects need to be updated since semanticdb only supports a subset of Scala versions) 
- `compile_exit_code` (int): exit code of running `sbt compileWithStats`
- `compile_duration` (int): how long did it take (in seconds)
- `compile_classes` (int): number of generated `*.class` files 
- `semanticdb_exit_code` (int): exit code of running `sbt semanticdb`
- `semanticdb_duration` (int): how long did it take (in seconds) 
- `semanticdb_classes` (int): number of generated `*.class` files
- `semanticdb_files` (int): number of generated `*.semanticdb` files
- `semanticdb_occurrences` (int): number of semanticdb occurrences (symbol occurrences)
- `semanticdb_synthetics` (int): number of semanticdb synthetics (what Scala compiler injects - implicits / desuggaring)
- `semanticdb_symbols` (int): number of semanticdb symbols
- `implicits_exit_code` (int): exit code of running the implicit extractor
- `implicits_duration` (int): how long did it take (in seconds)
- `implicit_cause_failure` (chr): the error message given in case of implicit extraction failure
- `declarations` (int): number of resolved declarations
- `implicit_declarations` (int): number of resolved implicit declarations used in the project
- `implicit_local_declarations` (int): number of implicit declarations defined in the project
- `callsites` (int): estimated number of all call sites
- `implicit_callsites` (int): number of call sites involving implicits (a conversion, implicit parameters, both) 
- `implicit_extraction_errors` (int): number of problems encountered when running the extrcator (e.g. improssible to resolve a symbol given a source code location) 
- `metadata_cause` (chr): the cause of metadata failure 
- `metadata_cause_detail` (chr): detail of the cause 
- `compile_cause` (chr): the cause of semanticdb failure 
- `compile_cause_detail` (chr): detail of the cause  
- `semanticdb_cause` (chr): the cause of semanticdb failure 
- `semanticdb_cause_detail` (chr): detail of the cause 

Exit code:
- `0`: all good
- `1`: failure
- `-1`: not run because of failed dependencies
- `>= 130`: timeouted

## Loading data

```{r loading data}
stage1_corpus <- read_csv(CORPUS_STAGE1)
stage2_corpus <- read_csv(CORPUS_STAGE3)
stage3_corpus <- filter_stage3_corpus(stage2_corpus)
```

The final corpus only contains projects for which the implicit extractor successfully ran and that have some compiled Scala code.

```{r final corpus}
corpus <- read_csv(CLEAN_CORPUS)
```

### Checks

In the final corpus, the metadata, semanticdb stages should have 0 exit code.

```{r check metadata}
corpus_failed_metadata <- corpus %>% filter(metadata_exit_code != 0)
assert_that(nrow(corpus_failed_metadata) == 0)
```

```{r check semanticdb}
corpus_failed_semanticdb <- corpus %>% filter(semanticdb_exit_code != 0)
assert_that(nrow(corpus_failed_semanticdb) == 0)
```

## Stage 3 Corpus Summary

The following is the result of the corpus after running the stage 2.

```{r projects overview}
compatible_projects <- filter(stage1_corpus, compatible)
duplicated_projects <- anti_join(compatible_projects, stage2_corpus, by="project_id")

overview_table(
  overview_projects("downloaded projects (stage 1)", stage1_corpus),
  overview_projects("sbt projects",                  filter(stage1_corpus, build_system=="sbt")),
  overview_projects("maven projects",                filter(stage1_corpus, build_system=="maven")),
  overview_projects("graddle projects",              filter(stage1_corpus, build_system=="graddle")),
  overview_projects("compatible sbt projects",       compatible_projects),
  overview_projects("duplicated",                    duplicated_projects),
  overview_projects("selected projects (stage 2)",   stage2_corpus),
  overview_projects("extracted projects (stage 3)",  stage3_corpus, code_column=metadata_scala_code)
)
```

```{r projects duplication}
compatible_big_projects <- filter(compatible_projects, scala_code > 5e4)
stage2_big_projects <- filter(stage2_corpus, scala_code > 5e4)

overview_table(
  r("compatilbe big projects", compatible_big_projects),
  r("stage two big projects", stage2_big_projects),
  r("compatible big projects duplication pct", percent(mean(compatible_big_projects$dejavu_duplication))),
  r("stage two big projects duplication pct", percent(mean(stage2_big_projects$dejavu_duplication))),
  r("lost stars in duplicated projects pct", percent(sum(duplicated_projects$gh_stars)/sum(compatible_projects$gh_stars))),
)
```

```{r corpus details}
extracted_metadata <- filter(stage2_corpus, metadata_scala_code > 0, metadata_exit_code == 0)
generated_semanticdb <- filter(extracted_metadata, semanticdb_exit_code == 0)

scala_code <- sum(corpus$metadata_scala_code)
scala_code_compile <- sum(corpus$metadata_scala_code_compile)
scala_gencode_compile <- sum(corpus$metadata_scala_code_compile_managed)
scala_code_test <- sum(corpus$metadata_scala_code_test)
scala_gencode_test <- sum(corpus$metadata_scala_code_test_managed)
scala_gencode <- scala_gencode_compile + scala_gencode_test

projects_with_tests <- nrow(filter(corpus, metadata_scala_code_test > 0))
projects_without_tests <- nrow(filter(corpus, metadata_scala_code_test == 0))

overview_table(
  overview_projects("extracted metadata",   extracted_metadata, code_column=metadata_scala_code),
  overview_projects("generated semanticdb", generated_semanticdb, code_column=metadata_scala_code),
  overview_projects("corpus", corpus, code_column=metadata_scala_code),

  r("corpus gencode", scala_gencode),
  r("corpus main code", scala_code_compile),
  r("corpus main gencode", scala_gencode_compile),
  r("corpus test code", scala_code_test),
  r("corpus test gencode", scala_gencode_test),
  r("corpus files", sum(corpus$scala_files)),
  r("corpus gencode pct", percent(scala_gencode/scala_code)),
  r("corpus test pct", percent(scala_code_test/scala_code)),
  r("corpus main gencode pct", percent(scala_gencode_compile/scala_code_compile)),
  r("corpus test gencode pct", percent(scala_gencode_test/scala_code_test)),
  r("corpus with test", projects_with_tests),
  r("corpus without test", projects_without_tests),
  r("corpus with test pct", percent(projects_with_tests/nrow(corpus))),
  r("corpus code median", median(corpus$metadata_scala_code)),
  r("corpus code mean", mean(corpus$metadata_scala_code)),
  r("corpus stars mean", mean(corpus$gh_stars)),
  r("corpus commits mean", mean(corpus$commit_count)),
  r("corpus big projects", filter(corpus, metadata_scala_code > 5e4)),
  r("corpus in scaladex", filter(corpus, scaladex)),
  r("corpus not in scaladex", filter(corpus, !scaladex)),
)
```

## Scala versions

```{r}
corpus_scala_versions <- 
  corpus %>%
  mutate(scala_major_version=substr(updated_scala_version, 1, 4))

corpus_2_11 <- filter(corpus_scala_versions, scala_major_version=="2.11")
corpus_2_12 <- filter(corpus_scala_versions, scala_major_version=="2.12")

overview_table(
  overview_projects("corpus scala eleven", corpus_2_11, code_column=metadata_scala_code),
  overview_projects("corpus scala twelve", corpus_2_12, code_column=metadata_scala_code),
  r("corpus scala eleven pct", percent(nrow(corpus_2_11)/nrow(corpus))),
  r("corpus scala eleven code pct", percent(sum(corpus_2_11$metadata_scala_code)/sum(corpus$metadata_scala_code))),
  r("corpus scala eleven stars pct", percent(sum(corpus_2_11$gh_stars)/sum(corpus$gh_stars))),
)

corpus_scala_versions <- corpus_scala_versions %>%
  select(scala_major_version, metadata_scala_code, gh_stars) %>%
  group_by(scala_major_version) %>%
  summarise(
    `Projects`=n(), 
    `Scala code`=oom(sum(metadata_scala_code)),
    `GitHub stars`=oom(sum(gh_stars))
  ) %>%
  mutate_all(fmt) %>%
  rename(
    `Scala Version`=scala_major_version
  )

my_datatable(corpus_scala_versions)
```

```{r}
corpus_scala_versions %>%
  xtable::xtable(align="lcrrr") %>%
  print(
    floating=F,
    file=OUT_SCALA_VERSIONS_TEX,
    include.rowname=F,
    sanitize.text.function=identity
  )
```

Notes:

```{r compiler benchmakr, include=FALSE}
compiler_benchmark <- filter(corpus, project_id=="scala--compiler-benchmark")
```

- For the final corpus we use `metadata_scala_code` instead of `scala_code`.
  The difference is that `scala_code` is computed using [cloc](https://sourceforge.net/projects/cloc/) with `--vcs=git` so it includes all the Scala files that are in the repository. 
  The `metadata_scala_code` on the other hand includes the code that was actually compiled. 
  It includes the source directories known to SBT (`sbt show sourceDirectories`) which could be a subset of what is stored in the project repository.
  Also it will include generated code.
  The good example is [compiler-benchmark](https://github.com/scala/compiler-benchmark) which has `r fmt(compiler_benchmark$scala_code)` lines of Scala code stored in git (`scala_code`), but only `r fmt(compiler_benchmark$metadata_scala_code)` is actually compiled (`metadata_scala_code`).
  The rest is code from various projects used for menchmarking scalac.

### Split Corpus

```{r}
filter(corpus, !scaladex) %>%
  ggplot(aes(x=forcats::fct_reorder(project_id, metadata_scala_code_compile), y=metadata_scala_code_compile)) +
  geom_point() +
  scale_y_log10(labels=scales::comma) +
  theme(
    axis.ticks.x = element_blank()
  )
```


```{r}
SMALL_BIG_APP_SEP <- 1000

tag("SmallBigAppSep", fmt(SMALL_BIG_APP_SEP))

split_corpus <- 
  corpus %>%
  select(
    project_id,
    origin,
    commit_count,
    dejavu_duplication,
    gh_stars,
    scaladex,
    scala_code=metadata_scala_code_compile,
    scala_test_code=metadata_scala_code_test,
    scala_version,
    updated_scala_version,
    declarations,
    explicit_callsites,
    explicit_test_callsites
  ) %>%
  mutate(
    explicit_callsites=explicit_callsites - explicit_test_callsites
  )

cat_tests <- 
  filter(split_corpus, scala_test_code > 0) %>% 
  mutate(
    cat="test", 
    scala_code=scala_test_code,
    explicit_callsites=explicit_test_callsites,
    commit_count=NA,
    gh_stars=NA
  )

cat_libs <- 
  filter(split_corpus, scaladex) %>% 
  mutate(cat="lib")

cat_apps_small <- 
  filter(split_corpus, !scaladex, scala_code <= SMALL_BIG_APP_SEP) %>% 
  mutate(cat="app_small")

cat_apps_big <- filter(split_corpus, !scaladex, scala_code > SMALL_BIG_APP_SEP) %>% 
  mutate(cat="app_big")

split_corpus <- bind_rows(
  cat_tests,
  cat_libs,
  cat_apps_small,
  cat_apps_big
) %>%
  select(-scala_test_code, -explicit_test_callsites)

overview_table(
  overview_projects("tests", cat_tests),
  overview_projects("libs", cat_libs),
  overview_projects("small apps", cat_apps_small),
  overview_projects("big apps", cat_apps_big),
)

stopifnot(nrow(anti_join(corpus, split_corpus, by="project_id")) == 0)
```

```{r add cat into corpus}
corpus <- 
  left_join(
    corpus,
    filter(split_corpus, cat != "test") %>% select(project_id, cat),
    by="project_id"
  )
```

```{r split overview}
category_overview <-
  split_corpus %>%
  select(
    cat, 
    scala_code, 
    gh_stars, 
    commit_count, 
#    explicit_callsites
  ) %>%
  group_by(cat) %>%
  mutate(projects=1) %>%
  summarise_all(list(sum=sum, median=median, mean=mean)) %>%
  select(-projects_median, -projects_mean) %>%
  mutate_at(vars(-cat), ~fmt(oom(.))) %>%
  arrange(desc(projects_sum)) %>%
  transmute(
    `Category`=fmt_categories(cat),
    `Projects`=projects_sum,
    `Code size`=str_c(scala_code_sum, " (mean=", scala_code_mean, ")"),
    `\\GH stars`=str_c(gh_stars_sum, " (mean=", gh_stars_mean, ")"),
    `Commits`=str_c(commit_count_sum, " (mean=", commit_count_mean, ")"),
#    `Explicit call sites`=str_c(explicit_callsites_sum, " (mean=", explicit_callsites_mean, ")")
  ) %>%
  mutate_all(~replace_na(., "-"))

my_datatable(category_overview)
```

```{r save category overview}
category_overview %>%
  xtable::xtable(align="llllll") %>%
  print(
    floating=F,
    file=OUT_CATEGORIES_TEX,
    include.rowname=F,
    sanitize.text.function=identity
  )
```

```{r save categories into file}
write_csv(split_corpus, CAT_CORPUS)
```

#### Graph

```{r corpus overview plot}
corpus_code_mean <- mean(corpus$metadata_scala_code_compile)
corpus_code_median <- median(corpus$metadata_scala_code_compile)
corpus_commit_mean <- mean(corpus$commit_count)
corpus_commit_median <- median(corpus$commit_count)

corpus %>%
  mutate(
    cat=cat_to_fct(cat)
  ) %>%
  ggplot(
    aes(
      x=metadata_scala_code_compile, 
      y=commit_count, 
      color=cat,
      size=gh_stars
    )
  ) +
  geom_point(alpha=.6) +
#  geom_hline(aes(yintercept=corpus_commit_median), linetype="dashed", color="black", size=0.2) +
#  geom_vline(aes(xintercept=corpus_code_median), linetype="dashed", color="black", size=0.2) +
  geom_hline(aes(yintercept=corpus_commit_mean), linetype="dashed", color="black", size=0.2) +
  geom_vline(aes(xintercept=corpus_code_mean), linetype="dashed", color="black", size=0.2) +
  geom_vline(aes(xintercept=SMALL_BIG_APP_SEP),linetype="solid", color="red", size=0.4) +

  scale_x_log10(labels = scales::comma) + 
  scale_y_log10(labels = scales::comma) +
  scale_size_continuous(range = c(.1, 8), labels = scales::comma) +
  # values=c("lib"="red", "app_big"="blue", "app_small"="lightblue"), 
  scale_color_wsj(labels=fmt_categories) +
  #scale_linetype_manual(labels=c("mean", "small/big separator"), values=c("dashed", "solid")) +

  theme(
    legend.position=c(0.2, 0.85),
    legend.box="horizontal",
    legend.box.background = element_rect(fill="white", size=0.1)
  ) +
  labs(
    x="Source lines of code excluding tests (log)", 
    y="Number of commits (log)", 
    color="Category",
    size="Github stars",
    linetype=""
  ) +
  guides(
    size=guide_legend(order=1),
    color=guide_legend(order=2),
    linetype=F
  )
```

```{r save corpus overview plot}
ggsave(OUT_CORPUS_OVERVIEW_PDF, width=9.88, height=7.93, device=cairo_pdf)
```

### TOP projects

```{r top projects table}
top_projects <- 
  corpus %>%
  filter(cat != "Test") %>%
  transmute(
    name=str_replace(project_id, "--", "/"),
    name=xtable::sanitize(name),
    name=str_c("\\href{https://github.com/", name, "/}{", name, "}"),
    gh_stars,
    metadata_scala_code_compile,
    commit_count,
    dejavu_duplication,
    scala_version,
    scaladex=if_else(scaladex, "Y", "N")
  ) %>%
  top_n(40, gh_stars) %>%
  arrange(desc(gh_stars)) %>%
  mutate_all(fmt) %>%
  rename(
    `Project`=name,
    `GitHub stars`=gh_stars,
    `Code size`=metadata_scala_code_compile,
    `Commits`=commit_count,
    `Duplication`=dejavu_duplication,
    `Scala version`=scala_version,
    `Scaladex`=scaladex
  )

my_datatable(top_projects)
```

```{r save top projects}
top_projects %>%
  xtable::xtable(align="llrrrrcc") %>%
  print(
    floating=F,
    file=OUT_TOP_PROJECTS_TEX,
    include.rowname=F,
    sanitize.text.function=identity
  )
```

## Pipeline

In stage 2 each project goes through the following phases:

1. **Metadata extraction.**
   This runs `sbt clean metadata` which is our SBT [plugin](https://github.com/PRL-PRG/scala-implicits-analysis/tree/oopsla19/sbt-plugins) that outputs relevant SBT configuration into a number of CSV files.
   Even though we are only interested in metadata, getting the full project dependenct classpath including inter-project dependencies will trigger a new compilation round.
   It has been suggested that there is a workaround for this (cf. [stackoverflow](https://stackoverflow.com/questions/53816695/how-to-get-dependencyclasspath-in-sbt-build-without-triggering-compilation)), but unfortunately that has side effects, namely missing source directories.

2. **Semanticdb generation.**
   This runs `sbt semanticdb` command which configures the projects scalac options to include the semanticdb [plugin](https://scalameta.org/docs/semanticdb/guide.html#scalac-compiler-plugin) with the option to output synthetics, i.e. pieces of AST added by the scala compiler which among other things are implicits.
  
3. **Implicits extraction.**
   Finally, we run our [tool](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/libs/tools/src/main/scala/cz/cvut/fit/prl/scala/implicits/tools/ExtractImplicits.scala) that takes semanticdb and project metadata and produce a [model](https://github.com/PRL-PRG/scala-implicits-analysis/blob/master/libs/model/src/main/protobuf/model.proto) of implicit declarations and call sites.
   The final model is stored in a Google protocol buffer format and the individual projects' models are merged into ``r GLOBAL_IMPLICITS`` file.

### Phase overview

Since we are working with real-world projects it is possible that not all of them will make it through all the phases.
The following table shows how many projects (Scala code and GitHub stars) do we loose in each step.

```{r}
phase_info <- function(phase, df, code_column=scala_code) {
  code_column <- enquo(code_column)

  df %>%
    summarise(
      `Phase`=phase,
      `Number of projects`=n(),
      `Scala code`=sum(!!code_column, na.rm=T),
      `GitHub Stars`=sum(gh_stars),
      `Repository size`=size(sum(size_repo, na.rm=T))
    ) %>%
    mutate_all(fmt)
}

corpus_building <- overview(
  phase_info("Downloaded projects", stage1_corpus),
  phase_info("\\SBT projects",      filter(stage1_corpus, build_system=="sbt")),
  phase_info("Compatible \\SBT projects",   filter(stage1_corpus, compatible)),
  phase_info("Duplicates",          anti_join(filter(stage1_corpus, compatible), stage2_corpus, by="project_id")),
  phase_info("Selected projects",        stage2_corpus),
  phase_info("After metadata extraction", filter(stage2_corpus, metadata_exit_code==0), code_column=metadata_scala_code),
  phase_info("After \\SDB generation",    filter(stage2_corpus, metadata_exit_code==0, semanticdb_exit_code==0), code_column=metadata_scala_code),
  phase_info("After implicit extraction", corpus, code_column=metadata_scala_code)
)

my_datatable(corpus_building)
```

```{r}
corpus_building %>%
  xtable::xtable(align="llrrrr") %>%
  print(
    floating=F,
    file=OUT_CORPUS_BUILDING_TEX,
    include.rowname=F,
    sanitize.text.function=identity
  )
```


For the figure

```{r}
phase_info <- function(phase, df, code_column=scala_code) {
  code_column <- enquo(code_column)

  df %>%
    summarise(
      `Phase`=phase,
      `Number of projects`=n(),
      `Scala code`=oom(sum(!!code_column, na.rm=T)),
      `GitHub Stars`=oom(sum(gh_stars)),
      `Repository size`=size(sum(size_repo, na.rm=T))
    ) %>%
    mutate_all(fmt)
}

corpus_building <- overview(
  phase_info("Downloaded projects", stage1_corpus),
  phase_info("Compatible \\SBT projects",   filter(stage1_corpus, compatible)),
  phase_info("Selected projects",        stage2_corpus),
  phase_info("After \\SDB generation",    filter(stage2_corpus, metadata_exit_code==0, semanticdb_exit_code==0), code_column=metadata_scala_code),
  phase_info("After implicit extraction", corpus, code_column=metadata_scala_code)
)

my_datatable(corpus_building)
```

### How much Spark do we have

It seems that people like to have their own version of Spark.
The problem is that Spark is big (the biggest Scala project excluding the JS stuff) so it will potetially skew the data.

The simple heuristics is that the name contains `spark` and the project has over 100K lines of code.
```{r}
filter_spark <- function(df) {
  df %>%
    filter(
      str_detect(tolower(project_id), "spark"), 
      scala_code > 1e5 # spark has over 100K SLOC
    )
}
```

In the corpus we have approximatelly:

```{r stage 1 spark overview}
spark_in_stage1 <- filter_spark(compatible_projects)

overview_table(
  overview_projects("spark projects before", spark_in_stage1),
  r("spark projects before code pct", percent(sum(spark_in_stage1$scala_code)/sum(compatible_projects$scala_code, na.rm=T)))
)
```

Who are they:

```{r spark projects in stage 1}
spark_in_stage1 %>%
  arrange(desc(gh_stars)) %>%
  my_datatable()
```

After filtering we get

```{r stage 2 spark overview}
spark_in_stage2 <- filter_spark(stage2_corpus)

overview_table(
  overview_projects("spark projects after", spark_in_stage2),
  r("spark projects after code pct", percent(sum(spark_in_stage2$scala_code)/sum(stage2_corpus$scala_code, na.rm=T)))
)
```

Who are they:

```{r spark projects in stage 2}
spark_in_stage2 %>% 
  arrange(desc(gh_stars)) %>%
  my_datatable()
```

### Errors

The following analysis shall help to identify the error that occur in the different phases.

This table shows the summary of exit codes in the different phases:

```{r}
phases <- c("metadata", "compile", "semanticdb", "implicits")
phases_data <- list(
  select(stage2_corpus, project_id, exit_code=metadata_exit_code),
  select(stage2_corpus, project_id, exit_code=compile_exit_code),
  select(stage2_corpus, project_id, exit_code=semanticdb_exit_code),
  select(stage2_corpus, project_id, exit_code=implicits_exit_code)
)
status <- map2_dfr(phases, phases_data, ~phase_status(.x, .y)) %>% mutate_at(vars(-phase), function(x) coalesce(x, 0))
status %>% my_datatable()
```

#### Failed metadata, but has semanticdb

These are errors should be investigated closely, since extracting metadata should in general have less problems that generating semanticdb.
However, with SBT no one really knows.
Usually the cause of the problem is that metadata does not compile the same modules.

```{r}
sdb_no_metadata <- 
  filter(stage2_corpus, metadata_exit_code != 0, semanticdb_exit_code == 0) %>%
  select(project_id, metadata_exit_code, metadata_failure, metadata_failure_detail, metadata_duration, metadata_scala_code, gh_stars)
```

```{r}
sdb_no_metadata %>%
  count(metadata_failure) %>% 
  my_datatable()
```

```{r}
sdb_no_metadata %>% 
  my_datatable()
```

### Metadata and semanticdb errors

This is to guess what has happened.
Execution of each phase is stored in a log file.
The log files are stored in `<project_directory>/_analysis_/<phase>.log` where phase is one if (`compile`, `metadata`, `semanticdb`, `implcits`).
We query these log files for a number of regular expressions that identify common failures.
They are decribed in the `[guess_failure_cause](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/scripts/inc/functions.R#L181)` function.

```{r}
failed_projects <- filter(
  stage2_corpus, 
  metadata_exit_code != 0 | (metadata_exit_code == 0 & semanticdb_exit_code > 0)
) %>%
  mutate(
    phase=case_when(
      metadata_exit_code == 1 ~ "metadata",
      semanticdb_exit_code == 1 ~ "semanticdb",
      TRUE ~ as.character(NA)
    ),
    cause=if_else(metadata_exit_code == 1, metadata_failure, semanticdb_failure),
    cause_detail=if_else(metadata_exit_code == 1, metadata_failure_detail, semanticdb_failure_detail),
    duration=if_else(phase=="metadata", metadata_duration, semanticdb_duration)
  ) %>%
  select(project_id, phase, cause, cause_detail, metadata_scala_code, gh_stars)
```

Summarized by exceptions:

```{r}
count(failed_projects, cause) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

```{r}
overview_table(
  r("implicit extraction errors",             sum(corpus$implicit_extraction_errors)),
  r("implicit extraction failures",           filter(corpus, !is.na(implicit_failure))),
  r("failed projects - all",                  anti_join(stage2_corpus, stage3_corpus, by="project_id")),
  r("failed projects - metadata",             filter(stage2_corpus, metadata_exit_code != 0)),
  r("failed projects - semanticdb",           filter(stage2_corpus, metadata_exit_code == 0, semanticdb_exit_code > 0)),
  r("failed projects - missing dependencies", filter(failed_projects, cause=="missing-dependencies")),
  r("failed projects - compile error",        filter(failed_projects, cause=="compilation-failed")),
  r("failed projects - broken build",         filter(failed_projects, cause=="project-loading-failed")),
  r("failed projects - empty build",          filter(stage2_corpus, implicits_exit_code==0, is.na(metadata_scala_code))),
  r("failed projects - missing scalajs",      filter(failed_projects, cause=="missing-dependencies", str_detect(cause_detail, "scalajs"))),
  r("failed projects - missing snapshots",    filter(failed_projects, cause=="missing-dependencies", str_detect(cause_detail, "SNAPSHOT")))
)
```

### Extractor code size

TODO: run cloc from here