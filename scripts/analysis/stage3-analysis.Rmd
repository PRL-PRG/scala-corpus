---
title: "Stage 3 Analysis"
authors: Filip Krikava, Jan Vitek and Heather Miller
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
params:
  base_dir: ../../../corpora/4-github
  lib_dir: ../inc
  base_url: http://prl1.ele.fit.cvut.cz:8149
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fs)
library(tidyverse)
library(ggplot2)
library(DT)
library(lubridate)
library(knitr)
library(glue)
library(pbapply)
library(xtable)
library(ggthemes)
library(feather)

theme_set(theme_minimal())

options(corpus_dir=params$base_dir)

source(path(params$lib_dir, "paths.R"))
source(path(params$lib_dir, "functions.R"))

pboptions(type="txt")
```

## Overview

The `r CORPUS_STAGE3_F` contains only the projects that were selected in stage 1.
It also contains details about the stage 2, i.e. the SBT metadata extraction, compilation and semanticdb generation.
In the stage 3, we filter the projects that have been successfully processed and which will be part of the final corpus.

The file contains the following columns:

- `project_id` (chr): project name as `github-user-name--github-repository-name`
- `origin` (chr): the URL to github 
- `build_system` (chr): guessed build system
- `sbt_version` (chr):  guessed sbt version
- `size_repo` (int): of the git versioned files in bytes
- `size` (int): of everything in bytes
- `commit_count` (int): number of commits
- `commit` (chr): current checkout hash
- `commit_date` (dttm): the date of the current commit
- `first_commit_date` (dttm): the date of the first commit
- `scala_code` (int): number of lines of Scala code excluding blanks and comments
- `scala_files` (int): number of Scala files
- `dejavu_n_files` (int): number of files processed by Dejavu (this shall be less than `scala_files` since it only consider non-token-empty ones with `.scala suffix`)
- `dejavu_n_duplicated_files` (int):  number of duplicate files (files seen in other projects)
- `dejavu_duplication` (dbl): `dejavu_n_duplicated_files/dejavu_n_files`
- `gh_name` (chr): GitHub name
- `gh_stars` (int): Number of stars 
- `gh_watchers` (int): Number of watchers
- `gh_created_at` (dttm): When was the project created at GitHub
- `gh_updated_at` (dttm): When was the project last updated at GitHub
- `gh_pushed_at` (dttm): When it has been last pushed to the project at GitHub
- `gh_fork` (lgl): Is it a fork?
- `gh_archived` (lgl): Has it been archived?
- `scaladex` (lgl): Has it been indexed by scaladex?
- `metadata_exit_code` (int): exit code of running `sbt metadata`
- `metadata_duration` (int): how long did it take (in seconds)
- `metadata_scala_code_test_managed` (int): managed Scala code in test
- `metadata_scala_code_test` (int): unmanaged and managed Scala code in test
- `metadata_scala_code_compile_managed` (int): managed Scala code in compile
- `metadata_scala_code_compile` (int): unmanaged and managed Scala code in compile
- `metadata_scala_code` (int): all Scala code above
- `modules` (int): number of subprojects
- `scala_version` (chr): scala version `sbt show scalaVersion` 
- `updated_scala_version` (chr): the scala version we used for semanticdb (some projects need to be updated since semanticdb only supports a subset of Scala versions) 
- `compile_exit_code` (int): exit code of running `sbt compileWithStats`
- `compile_duration` (int): how long did it take (in seconds)
- `compile_classes` (int): number of generated `*.class` files 
- `semanticdb_exit_code` (int): exit code of running `sbt semanticdb`
- `semanticdb_duration` (int): how long did it take (in seconds) 
- `semanticdb_classes` (int): number of generated `*.class` files
- `semanticdb_files` (int): number of generated `*.semanticdb` files
- `semanticdb_occurrences` (int): number of semanticdb occurrences (symbol occurrences)
- `semanticdb_synthetics` (int): number of semanticdb synthetics (what Scala compiler injects - implicits / desuggaring)
- `semanticdb_symbols` (int): number of semanticdb symbols
- `implicits_exit_code` (int): exit code of running the implicit extractor
- `implicits_duration` (int): how long did it take (in seconds)
- `implicit_cause_failure` (chr): the error message given in case of implicit extraction failure
- `declarations` (int): number of resolved declarations
- `implicit_declarations` (int): number of resolved implicit declarations used in the project
- `implicit_local_declarations` (int): number of implicit declarations defined in the project
- `callsites` (int): estimated number of all call sites
- `implicit_callsites` (int): number of call sites involving implicits (a conversion, implicit parameters, both) 
- `implicit_extraction_errors` (int): number of problems encountered when running the extrcator (e.g. improssible to resolve a symbol given a source code location) 
- `metadata_cause` (chr): the cause of metadata failure 
- `metadata_cause_detail` (chr): detail of the cause 
- `compile_cause` (chr): the cause of semanticdb failure 
- `compile_cause_detail` (chr): detail of the cause  
- `semanticdb_cause` (chr): the cause of semanticdb failure 
- `semanticdb_cause_detail` (chr): detail of the cause 

Exit code:
- `0`: all good
- `1`: failure
- `-1`: not run because of failed dependencies
- `>= 130`: timeouted

## Loading data

```{r loading data}
stage1_corpus <- read_feather(CORPUS_STAGE1_F)
stage3_corpus <- read_feather(CORPUS_STAGE3_F)

corpus <- filter(stage3_corpus, implicits_exit_code==0, metadata_scala_code > 0) 
```

## Stage 3 Corpus Summary

The difference between _Scala SLOC_ and _SBT Scala SLOC_ is that the former one is computed using [cloc](https://sourceforge.net/projects/cloc/) with `--vcs=git` so it includes all the Scala files that are in the repository.
The latter on the other hand only includes the source directories known to SBT (`sbt show sourceDirectories`).

The _Analyzed SBT_ is the final number of projects/code that we successfully run the entire pipeline (using the SBT Scala SLOC).

```{r projects stats}
make_stats(
  add_num("Scala code", stage3_corpus$scala_code),
  add_num("Scala files", stage3_corpus$scala_files),
  add_num("Compiled Scala code", stage3_corpus$metadata_scala_code),
  add_num("Compiled Scala test code", stage3_corpus$metadata_scala_code_test),
  add_num("Github Stars", stage3_corpus$gh_stars),
  add_nrow("Extracted metadata", filter(stage3_corpus, metadata_exit_code==0)),
  add_nrow("Compiled", filter(stage3_corpus, compile_exit_code==0)),
  add_nrow("Extracted semanticdb", filter(stage3_corpus, semanticdb_exit_code==0)),
  add_nrow("Extracted implicits", filter(stage3_corpus, implicits_exit_code==0)),
  add_nrow("Analyzed projects (some are empty)", corpus),
  add_num("Analyzed projects rounded (some are empty)", 6500),
  add_num("Analyzed Scala code (compiled and successfully extracted)", corpus$metadata_scala_code),
  add_num("Analyzed Scala code rounded", round(sum(corpus$metadata_scala_code)/1e6, 1), suffix="M"),
  add_num("Analyzed Scala generated code (managed)", corpus$metadata_scala_code_compile_managed+corpus$metadata_scala_code_test_managed),
  add_num("Analyzed Scala test code", corpus$metadata_scala_code_test),
  add_num("Analyzed Scala generated test code (managed", corpus$metadata_scala_code_test_managed),
  add_num("Analyzed commits", corpus$commit_count),
  add_num("Analyzed Github stars", corpus$gh_stars),
  add_nrow("Analyzed libraries", filter(corpus, scaladex)),
  add_nrow("Analyzed Applications", filter(corpus, !scaladex)),
  add_num("Implicit declarations", corpus$implicit_declarations),
  add_num("Local implicit declarations", corpus$implicit_local_declarations),
  add_num("Implicit call sites", corpus$implicit_callsites),
  add_num("Call sites", corpus$callsites),
  add_num("Errors in implicit extraction", corpus$implicit_extraction_errors),
  add_nrow("Implicits extraction failures", filter(corpus, !is.na(implicit_failure)))
) %>%
  my_datatable()
```

```{r corpus overview plot}
corpus_code_mean <- mean(corpus$metadata_scala_code)
corpus_code_median <- median(corpus$metadata_scala_code)
corpus_commit_mean <- mean(corpus$commit_count)
corpus_commit_median <- median(corpus$commit_count)

corpus %>%
  mutate(
#    outlier=is_outlier(metadata_scala_code)|is_outlier(commit_count)|is_outlier(gh_stars),
    outlier=gh_stars>=5000,
#    outlier=FALSE,
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=metadata_scala_code, 
      y=commit_count, 
      label=label, 
      color=if_else(scaladex, "Yes", "No"),
      size=gh_stars
    )
  ) +
    geom_point(alpha=.6) +
    geom_text(size=2.5, check_overlap = F, vjust=-1, na.rm = TRUE, color="black") + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    scale_size_continuous(range = c(.1, 8)) +
    scale_color_gdocs() +
  
    geom_hline(aes(yintercept=corpus_commit_mean, linetype="dashed"), color="black", size=0.2) +
    geom_vline(aes(xintercept=corpus_code_mean, linetype="dashed"), color="black", size=0.2) +
    annotate("text", x=1, y=corpus_commit_mean, label=floor(corpus_commit_mean), hjust=2.4, vjust=-1.0, size=3) + 
    annotate("text", x=corpus_code_mean, y=1, label=floor(corpus_code_mean), hjust=-.3, vjust=2.4, size=3) + 
  
    geom_hline(aes(yintercept=corpus_commit_median, linetype="solid"), color="black", size=0.2) +
    geom_vline(aes(xintercept=corpus_code_median, linetype="solid"), color="black", size=0.2) +
    annotate("text", x=1, y=corpus_commit_median, label=floor(corpus_commit_median), hjust=3.2, vjust=-1.0, size=3) + 
    annotate("text", x=corpus_code_median, y=1, label=floor(corpus_code_median), hjust=-.3, vjust=2.4, size=3) + 
    scale_linetype_manual(labels=c("mean", "median"), values=c("dashed", "solid")) +
    theme(
      legend.position=c(0.25, 0.89),
      legend.box="horizontal"
    ) +
    labs(
      x="Souce lines of code (log)", 
      y="Number of commits (log)", 
      color="In Scaladex",
      size="Github stars",
      linetype=""
    )
```

## Pipeline

This is to show how much do we loose in each phase.

```{r}
corpus_shrinking <- tibble(phase=character(0), projects=integer(0), sloc=integer(0), stars=integer(0), size=integer(0))
add_phase <- function(phase, df, sloc="metadata_scala_code", stars="gh_stars", size="size_repo") {
  corpus_shrinking <<- add_row(
    corpus_shrinking, 
    phase=phase, 
    projects=nrow(df),
    sloc=ifelse(nrow(df) > 0, sum(df[, sloc], na.rm=T), 0),
    stars=ifelse(nrow(df) > 0, sum(df[, stars], na.rm=T), 0), 
    size=format_size(ifelse(nrow(df) > 0, sum(df[, size], na.rm=T), 0))
  )
}

add_phase("All projects", stage3_corpus)
add_phase("Metadata",     filter(stage3_corpus, metadata_exit_code==0))
add_phase("Compile",      filter(stage3_corpus, compile_exit_code==0))
add_phase("Semanticdb",   filter(stage3_corpus, semanticdb_exit_code==0))
add_phase("Implicits",    filter(stage3_corpus, implicits_exit_code==0))
add_phase("Analyzed",     corpus)

corpus_shrinking %>% 
  my_datatable()
```

```{r}
corpus_shrinking <- tibble(phase=character(0), projects=integer(0), sloc=integer(0), stars=integer(0), size=integer(0))

add_phase("Cloned projects", stage1_corpus, "scala_code")
add_phase("Compatible SBT projects", filter(stage1_corpus, compatible), "scala_code")
add_phase("Removed duplicate projects", anti_join(filter(stage1_corpus, compatible), stage3_corpus, by="project_id"), "scala_code")
add_phase("Without duplicates", stage3_corpus, "scala_code")
add_phase("Compiled", filter(stage3_corpus, semanticdb_exit_code==0), size="size")
add_phase("Extracted implicits", filter(stage3_corpus, implicits_exit_code==0), size="size")
add_phase("Analyzed", corpus, size="size")

corpus_shrinking %>% 
  my_datatable()

corpus_shrinking <- mutate_all(corpus_shrinking, fmt)
colnames(corpus_shrinking) <- c("", "Projects", "Scala Code", "GitHub Stars")
print(
  xtable(corpus_shrinking, align="llrrrr", label="tab:CorpusBuilding", caption="Overview of the corpus building"),
  file=get_corpus_path("table-corpus-building.tex"),
  include.rownames=FALSE, 
  comment=FALSE, 
  timestamp=FALSE
)
```

### Duplicate projects

```{r duplicate projects}
removed_duplicates <- anti_join(filter(stage1_corpus, compatible), stage3_corpus, by="project_id")
make_stats(
  add_nrow("Removed duplicate projects", removed_duplicates),
  add_num("Removed duplicate projects code", sum(removed_duplicates$scala_code)),
  add_num("Removed duplicate projects code rounded", round(sum(removed_duplicates$scala_code)/1e6, 1), suffix="M"),
  add_num("Removed duplicate projects stars", round(sum(removed_duplicates$gh_stars)/1e3, 1), suffix="K"),
  add_num("Removed duplicate projects size", sum(removed_duplicates$size_repo), formatter=format_size)
)
```


## Pipeline Errors

```{r}
phases <- c("metadata", "compile", "semanticdb", "implicits")
phases_data <- list(
  select(stage3_corpus, project_id, exit_code=metadata_exit_code),
  select(stage3_corpus, project_id, exit_code=compile_exit_code),
  select(stage3_corpus, project_id, exit_code=semanticdb_exit_code),
  select(stage3_corpus, project_id, exit_code=implicits_exit_code)
)
status <- map2_dfr(phases, phases_data, ~phase_status(.x, .y)) %>% mutate_at(vars(-phase), function(x) coalesce(x, 0))
status %>% my_datatable()
```

### Failed metadata, but has semanticdb

These are kind of suspicious, since metadata should be less involved than semanticdb.
However, with SBT no one really knows.

```{r}
sdb_no_metadata <- filter(stage3_corpus, metadata_exit_code != 0, semanticdb_exit_code == 0) %>%
  select(project_id, metadata_exit_code, metadata_failure, metadata_failure_detail, metadata_duration, metadata_scala_code, gh_stars)
```

```{r}
sdb_no_metadata %>%
  count(metadata_failure) %>% 
  my_datatable()
```

```{r}
sdb_no_metadata %>% 
  my_datatable()
```

### Metadata and semanticdb errors

This is to guess what has happened.

```{r}
failed_projects <- filter(
  stage3_corpus, 
  metadata_exit_code == 1 | 
    (metadata_exit_code == 0 & semanticdb_exit_code == 1)
) %>%
  mutate(
    phase=case_when(
      metadata_exit_code == 1 ~ "metadata",
      semanticdb_exit_code == 1 ~ "semanticdb",
      TRUE ~ as.character(NA)
    ),
    cause=if_else(metadata_exit_code == 1, metadata_failure, semanticdb_failure),
    cause_detail=if_else(metadata_exit_code == 1, metadata_failure_detail, semanticdb_failure_detail),
    duration=if_else(phase=="metadata", metadata_duration, semanticdb_duration)
  ) %>%
  select(project_id, phase, cause, cause_detail, metadata_scala_code, gh_stars)
```

Summarized by exceptions:

```{r}
count(failed_projects, cause) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

```{r}
make_stats(
  add_nrow("Projects failed to build", filter(stage3_corpus, implicits_exit_code!=0 | is.na(metadata_scala_code))),
  add_nrow("Projects missing dependencies", filter(failed_projects, cause=="missing-dependencies")),
  add_nrow("Projects with compile error", filter(failed_projects, cause=="compilation-failed")),
  add_nrow("Projects with broken build", filter(failed_projects, cause=="project-loading-failed")),
  add_nrow("Projects with empty build", filter(stage3_corpus, implicits_exit_code==0, is.na(metadata_scala_code))),
  add_nrow("Projects missing Scala JS", filter(failed_projects, cause=="missing-dependencies", str_detect(cause_detail, "scalajs"))),
  add_nrow("Projects missing Snapshot Dependencies", filter(failed_projects, cause=="missing-dependencies", str_detect(cause_detail, "SNAPSHOT")))
)
```


#### Which dependencies

It only considers the first dependency.

```{r}
count(filter(failed_projects, cause=="missing-dependencies"), cause_detail) %>% arrange(desc(n)) %>% my_datatable(colnames=c("Dependency", "Count"))
```

The scalajs is possibly because of some security vulnerability and the projects could just be updated.

#### List of Java errors

```{r}
filter(failed_projects, cause=="java-error") %>%
  mutate(
    log=make_corpus_link(path(project_id, "_analysis_", str_c(phase, ".log")))
  ) %>%
  select(-phase, -cause) %>%
  select(project_id, log, everything()) %>%
  my_datatable(escape=FALSE)
```


#### List of unknown problems

This is just to see of peraps there is something that could be done about it.

```{r}
filter(failed_projects, cause=="unknown") %>%
  mutate(
    log=make_corpus_link(path(project_id, "_analysis_", str_c(phase, ".log")))
  ) %>%
  select(-phase, -cause) %>%
  select(project_id, log, everything()) %>%
  my_datatable(escape=FALSE)
```

### Time of the successfull projects

```{r}
make_stats(
  add_num("Metadata extraction time", corpus$metadata_duration),
  add_num("Compile time", corpus$compile_duration),
  add_num("Semanticdb extraction time", corpus$semanticdb_duration),
  add_num("Implicits extraction time", corpus$implicits_duration)
) %>%
  my_datatable()
```

## Semanticdb

```{r}
filter(corpus, semanticdb_exit_code==0, is.na(semanticdb_files) | semanticdb_files==0) %>%
  mutate(path=make_corpus_link(project_id)) %>%
  select(project_id, path, metadata_exit_code, scala_version, commit_count, gh_stars, metadata_scala_code) %>%
  my_datatable(escape=FALSE)
```

## Errors in implicit extraction

### Failures

Failures when trying to extract implicits from semanticdb.
The `ZipException: error in opening zip file` means that it could not open a a dependency jar file.

```{r}
filter(corpus, !is.na(implicit_failure)) %>%
  select(project_id, implicit_failure, metadata_scala_code, gh_stars) %>%
  my_datatable()
```

### Errors

When extracting implicit declarations and call sites, there can be a number of problems.
Each of the gets logged into an `r IMPLICITS_EXCEPTIONS` files.
Here we try to clasify them.

```{r load implicit errors}
exceptions <- read_feather(IMPLICIT_EXCEPTIOSN_F)
```

```{r}
classified_exceptions <- 
  mutate(
    exceptions,
    class=case_when(
      exception == "SymbolNotFoundException" & str_detect(message, "local\\d+") ~ "missing-local-symbol",
      exception == "SymbolNotFoundException" & str_detect(message, "symbol: local\\d+") ~ "missing-local-symbol",
      exception == "SymbolNotFoundException" & str_detect(message, "at Range\\(\\d+,\\d+,\\d+,\\d+\\)") ~ "missing-symbol-at-range",
      exception == "SymbolNotFoundException" ~ "missing-symbol-other",
      exception == "LoadingMetadataException" & str_detect(message, "No module found") & str_detect(message, "multi-jvm") ~ "missing-module-multijvm",
      exception == "LoadingMetadataException" & str_detect(message, "No module found") ~ "missing-module",
      exception == "LoadingMetadataException" ~ "metadata-loading",
      exception == "NotImplementedError" ~ "not-implemented",
      exception == "UnexpectedElementException" ~ "unexpected-element",
      exception == "UnsupportedElementException" ~ "unsupported-element",
      exception == "UnExtractableCallSiteException" & cause == "SymbolNotFoundException" ~ "unextractable-callsite-missing-term",
      exception == "UnExtractableCallSiteException" & cause == "FunctionNotFoundException" ~ "unextractable-callsite-missing-function",
      exception == "UnExtractableCallSiteException" ~ "unextractable-callsite-other",
      exception == "ImplicitArgumentNotFoundException" ~ "missing-implicit-arguments",
      exception == "Exception" & str_detect(message, "^Invalid call site argument") ~ "invalid-declaration",
      TRUE ~ "unclassified"
    )
  )
```

#### Summary per project summary

```{r}
count(classified_exceptions, project_id) %>% 
  left_join(select(corpus, project_id, metadata_scala_code, gh_stars), by="project_id") %>%
  arrange(desc(n)) %>% 
  my_datatable() 
```

#### Summary per class summary

```{r}
count(classified_exceptions, class) %>% arrange(desc(n)) %>% my_datatable()
```

#### Summary per project and class summary

```{r}
count(classified_exceptions, project_id, class) %>% arrange(desc(n)) %>% my_datatable()
```

#### Missing symbols

##### Local

```{r}
filter(classified_exceptions, class=="missing-local-symbol") %>% 
  count(project_id) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

##### Range

```{r}
filter(classified_exceptions, class=="missing-symbol-at-range") %>% 
  count(project_id) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

##### Other

```{r}
filter(classified_exceptions, class=="missing-symbol-other") %>% 
  count(project_id) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

```{r}
filter(classified_exceptions, class=="missing-symbol-other") %>% 
  transmute(project_id, symbol=str_replace_all(message, ".*symbol: (.*)$", "\\1")) %>%
  count(project_id, symbol) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

#### Unclassified

```{r}
filter(classified_exceptions, class=="unclassified") %>%
  select(project_id, module_id, class, everything()) %>%
  my_datatable()
```

### Callsites and implicit callsites

```{r}
filter(corpus, implicits_exit_code==0, metadata_scala_code > 100) %>%
  mutate(
    outlier=is_outlier(callsites)|is_outlier(implicit_callsites),
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=callsites, 
      y=implicit_callsites, 
      label=label, 
      color=if_else(scaladex, "Library", "Application")
    )
  ) +
    geom_point(size=.5) +
    geom_text(size=3, check_overlap = T, vjust=1.5, na.rm = TRUE) + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    geom_vline(aes(xintercept=mean(callsites)), linetype=2, color="black", size=.2) + 
    geom_hline(aes(yintercept=mean(implicit_callsites)), linetype=2, color="black", size=.2) +
    labs(
      x="Number of callsites (log)", 
      y="Number of implicit callsites (log)", 
      color="Project type"
    )
```

```{r}
filter(corpus, implicits_exit_code==0, metadata_scala_code>0) %>%
  mutate(
    outlier=is_outlier(metadata_scala_code)|is_outlier(implicit_local_declarations),
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=metadata_scala_code, 
      y=implicit_local_declarations, 
      label=label, 
      color=if_else(scaladex, "Library", "Application")
    )
  ) +
    geom_point(size=.5) +
    geom_text(size=3, check_overlap = T, vjust=1.5, na.rm = TRUE) + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    geom_vline(aes(xintercept=mean(metadata_scala_code)), linetype=2, color="black", size=.2) + 
    geom_hline(aes(yintercept=mean(implicit_local_declarations)), linetype=2, color="black", size=.2) +
    labs(
      x="Source lines of code (log)", 
      y="Number of local declarations (log)", 
      color="Project type"
    )
```
