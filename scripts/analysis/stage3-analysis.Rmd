---
title: "Stage 3 Analysis"
authors: Filip Krikava, Jan Vitek and Heather Miller
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
params:
  base_dir: ../../../corpora/4-github
  lib_dir: ../inc
  base_url: http://prl1.ele.fit.cvut.cz:8149
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fs)
library(tidyverse)
library(ggplot2)
library(DT)
library(lubridate)
library(knitr)
library(glue)
library(pbapply)
library(xtable)
library(ggthemes)
library(feather)

theme_set(theme_minimal())

options(corpus_dir=params$base_dir)

source(path(params$lib_dir, "paths.R"))
source(path(params$lib_dir, "functions.R"))

pboptions(type="txt")
```

## Overview

The ``r CORPUS_STAGE3_F`` contains only the projects that were selected in stage 1.
Next to the data collected in ``r CORPUS_STAGE1_F``, it contains details about the stage 2, i.e. the project compilation, SBT metadata extraction, semanticdb generation and implicit extraction.
The goal of the stage 3 is to filter the projects that have been successfully processed and which will be part of the final corpus.

The file contains the following columns:

- `project_id` (chr): project name as `github-user-name--github-repository-name`
- `origin` (chr): the URL to github 
- `build_system` (chr): guessed build system
- `sbt_version` (chr):  guessed sbt version
- `size_repo` (int): of the git versioned files in bytes
- `size` (int): of everything in bytes
- `commit_count` (int): number of commits
- `commit` (chr): current checkout hash
- `commit_date` (dttm): the date of the current commit
- `first_commit_date` (dttm): the date of the first commit
- `scala_code` (int): number of lines of Scala code excluding blanks and comments
- `scala_files` (int): number of Scala files
- `dejavu_n_files` (int): number of files processed by Dejavu (this shall be less than `scala_files` since it only consider non-token-empty ones with `.scala suffix`)
- `dejavu_n_duplicated_files` (int):  number of duplicate files (files seen in other projects)
- `dejavu_duplication` (dbl): `dejavu_n_duplicated_files/dejavu_n_files`
- `gh_name` (chr): GitHub name
- `gh_stars` (int): Number of stars 
- `gh_watchers` (int): Number of watchers
- `gh_created_at` (dttm): When was the project created at GitHub
- `gh_updated_at` (dttm): When was the project last updated at GitHub
- `gh_pushed_at` (dttm): When it has been last pushed to the project at GitHub
- `gh_fork` (lgl): Is it a fork?
- `gh_archived` (lgl): Has it been archived?
- `scaladex` (lgl): Has it been indexed by scaladex?
- `metadata_exit_code` (int): exit code of running `sbt metadata`
- `metadata_duration` (int): how long did it take (in seconds)
- `metadata_scala_code_test_managed` (int): managed Scala code in test
- `metadata_scala_code_test` (int): unmanaged and managed Scala code in test
- `metadata_scala_code_compile_managed` (int): managed Scala code in compile
- `metadata_scala_code_compile` (int): unmanaged and managed Scala code in compile
- `metadata_scala_code` (int): all Scala code above
- `modules` (int): number of subprojects
- `scala_version` (chr): scala version `sbt show scalaVersion` 
- `updated_scala_version` (chr): the scala version we used for semanticdb (some projects need to be updated since semanticdb only supports a subset of Scala versions) 
- `compile_exit_code` (int): exit code of running `sbt compileWithStats`
- `compile_duration` (int): how long did it take (in seconds)
- `compile_classes` (int): number of generated `*.class` files 
- `semanticdb_exit_code` (int): exit code of running `sbt semanticdb`
- `semanticdb_duration` (int): how long did it take (in seconds) 
- `semanticdb_classes` (int): number of generated `*.class` files
- `semanticdb_files` (int): number of generated `*.semanticdb` files
- `semanticdb_occurrences` (int): number of semanticdb occurrences (symbol occurrences)
- `semanticdb_synthetics` (int): number of semanticdb synthetics (what Scala compiler injects - implicits / desuggaring)
- `semanticdb_symbols` (int): number of semanticdb symbols
- `implicits_exit_code` (int): exit code of running the implicit extractor
- `implicits_duration` (int): how long did it take (in seconds)
- `implicit_cause_failure` (chr): the error message given in case of implicit extraction failure
- `declarations` (int): number of resolved declarations
- `implicit_declarations` (int): number of resolved implicit declarations used in the project
- `implicit_local_declarations` (int): number of implicit declarations defined in the project
- `callsites` (int): estimated number of all call sites
- `implicit_callsites` (int): number of call sites involving implicits (a conversion, implicit parameters, both) 
- `implicit_extraction_errors` (int): number of problems encountered when running the extrcator (e.g. improssible to resolve a symbol given a source code location) 
- `metadata_cause` (chr): the cause of metadata failure 
- `metadata_cause_detail` (chr): detail of the cause 
- `compile_cause` (chr): the cause of semanticdb failure 
- `compile_cause_detail` (chr): detail of the cause  
- `semanticdb_cause` (chr): the cause of semanticdb failure 
- `semanticdb_cause_detail` (chr): detail of the cause 

Exit code:
- `0`: all good
- `1`: failure
- `-1`: not run because of failed dependencies
- `>= 130`: timeouted

## Loading data

First, we need to load the data.

```{r loading data}
stage1_corpus <- read_feather(CORPUS_STAGE1_F)
stage3_corpus <- read_feather(CORPUS_STAGE3_F)
```

The final corpus only contains projects for which the implicit extractor successfully ran and that have some compiled Scala code.

```{r}
corpus <- filter(stage3_corpus, implicits_exit_code==0, metadata_scala_code > 0) 
```

## Stage 3 Corpus Summary

The following is the result of the corpus after running the stage 2.

```{r projects stats}
make_stats(
  add_num("Scala code", stage3_corpus$scala_code),
  add_num("Scala files", stage3_corpus$scala_files),
  add_num("Compiled Scala code", stage3_corpus$metadata_scala_code),
  add_num("Compiled Scala test code", stage3_corpus$metadata_scala_code_test),
  add_num("Github Stars", stage3_corpus$gh_stars),
  add_nrow("Extracted metadata", filter(stage3_corpus, metadata_exit_code==0)),
  add_nrow("Compiled", filter(stage3_corpus, compile_exit_code==0)),
  add_nrow("Extracted semanticdb", filter(stage3_corpus, semanticdb_exit_code==0)),
  add_nrow("Extracted implicits", filter(stage3_corpus, implicits_exit_code==0)),
  add_nrow("Analyzed projects (non-empty)", corpus),
  add_num("Analyzed Scala code (compiled and successfully extracted)", corpus$metadata_scala_code),
  add_num("Analyzed Scala generated code (managed)", corpus$metadata_scala_code_compile_managed+corpus$metadata_scala_code_test_managed),
  add_num("Analyzed Scala test code", corpus$metadata_scala_code_test),
  add_num("Analyzed Scala generated test code (managed)", corpus$metadata_scala_code_test_managed),
  add_num("Analyzed commits", corpus$commit_count),
  add_num("Analyzed GitHub stars", corpus$gh_stars),
  add_nrow("Analyzed scaladex indexed projects (libraries)", filter(corpus, scaladex)),
  add_nrow("Analyzed non-scaladex indexed projects (applications)", filter(corpus, !scaladex)),
  add_num("Implicit declarations", corpus$implicit_declarations),
  add_num("Local implicit declarations", corpus$implicit_local_declarations),
  add_num("Implicit call sites", corpus$implicit_callsites),
  add_num("Call sites", corpus$callsites),
  add_num("Errors in implicit extraction", corpus$implicit_extraction_errors),
  add_nrow("Implicits extraction failures", filter(corpus, !is.na(implicit_failure)))
) %>%
  my_datatable()
```

Notes:

- The difference between _Scala code_ and _Analyzed Scala code_ is that the former one is computed using [cloc](https://sourceforge.net/projects/cloc/) with `--vcs=git` so it includes all the Scala files that are in the repository. The latter on the other hand only includes the source directories known to SBT (`sbt show sourceDirectories`).

### Graphical overview

```{r corpus overview plot}
corpus_code_mean <- mean(corpus$metadata_scala_code)
corpus_code_median <- median(corpus$metadata_scala_code)
corpus_commit_mean <- mean(corpus$commit_count)
corpus_commit_median <- median(corpus$commit_count)

corpus %>%
  mutate(
#    outlier=is_outlier(metadata_scala_code)|is_outlier(commit_count)|is_outlier(gh_stars),
    outlier=gh_stars>=5000,
#    outlier=FALSE,
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=metadata_scala_code, 
      y=commit_count, 
      label=label, 
      color=if_else(scaladex, "Yes", "No"),
      size=gh_stars
    )
  ) +
    geom_point(alpha=.6) +
    geom_text(size=2.5, check_overlap = F, vjust=-1, na.rm = TRUE, color="black") + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    scale_size_continuous(range = c(.1, 8)) +
    scale_color_gdocs() +
  
    geom_hline(aes(yintercept=corpus_commit_mean, linetype="dashed"), color="black", size=0.2) +
    geom_vline(aes(xintercept=corpus_code_mean, linetype="dashed"), color="black", size=0.2) +
    #annotate("text", x=1, y=corpus_commit_mean, label=floor(corpus_commit_mean), hjust=2.4, vjust=-1.0, size=3) + 
    #annotate("text", x=corpus_code_mean, y=1, label=floor(corpus_code_mean), hjust=-.3, vjust=2.4, size=3) + 
  
    geom_hline(aes(yintercept=corpus_commit_median, linetype="solid"), color="black", size=0.2) +
    geom_vline(aes(xintercept=corpus_code_median, linetype="solid"), color="black", size=0.2) +
    #annotate("text", x=1, y=corpus_commit_median, label=floor(corpus_commit_median), hjust=3.2, vjust=-1.0, size=3) + 
    #annotate("text", x=corpus_code_median, y=1, label=floor(corpus_code_median), hjust=-.3, vjust=2.4, size=3) + 
    scale_linetype_manual(labels=c("mean", "median"), values=c("dashed", "solid")) +
    theme(
      legend.position=c(0.25, 0.89),
      legend.box="horizontal"
    ) +
    labs(
      x="Souce lines of code (log)", 
      y="Number of commits (log)", 
      color="In Scaladex",
      size="Github stars",
      linetype=""
    )
```

### Duplicate projects

The following table show the number of projects (Scala code and GitHub stars) that were removed based on our duplication thresolds.

```{r duplicate projects}
removed_duplicates <- anti_join(filter(stage1_corpus, compatible), stage3_corpus, by="project_id")
make_stats(
  add_nrow("Removed duplicate projects", removed_duplicates),
  add_num("Removed duplicate projects code", sum(removed_duplicates$scala_code)),
  add_num("Removed duplicate projects stars", sum(removed_duplicates$gh_stars)),
  add_num("Removed duplicate projects size", sum(removed_duplicates$size_repo), formatter=format_size)
) %>% my_datatable()
```

## Pipeline

In stage 2 each project goes through the following phases:

1. **Metadata extraction.**
   This runs `sbt clean metadata` which is our SBT [plugin](https://github.com/PRL-PRG/scala-implicits-analysis/tree/oopsla19/sbt-plugins) that outputs relevant SBT configuration into a number of CSV files.
   Even though we are only interested in metadata, getting the full project dependenct classpath including inter-project dependencies will trigger a new compilation round.
   It has been suggested that there is a workaround for this (cf. [stackoverflow](https://stackoverflow.com/questions/53816695/how-to-get-dependencyclasspath-in-sbt-build-without-triggering-compilation)), but unfortunately that has side effects, namely missing source directories.

2. **Semanticdb generation.**
   This runs `sbt semanticdb` command which configures the projects scalac options to include the semanticdb [plugin](https://scalameta.org/docs/semanticdb/guide.html#scalac-compiler-plugin) with the option to output synthetics, i.e. pieces of AST added by the scala compiler which among other things are implicits.
  
3. **Implicits extraction.**
   Finally, we run our [tool](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/libs/tools/src/main/scala/cz/cvut/fit/prl/scala/implicits/tools/ExtractImplicits.scala) that takes semanticdb and project metadata and produce a [model](https://github.com/PRL-PRG/scala-implicits-analysis/blob/master/libs/model/src/main/protobuf/model.proto) of implicit declarations and call sites.
   The final model is stored in a Google protocol buffer format and the individual projects' models are merged into ``r GLOBAL_IMPLICITS`` file.

### Phase overview

Since we are working with real-world projects it is possible that not all of them will make it through all the phases.
The following table shows how many projects (Scala code and GitHub stars) do we loose in each step.

```{r}
corpus_shrinking <- tibble(phase=character(0), projects=integer(0), sloc=integer(0), stars=integer(0), size=integer(0))
add_phase <- function(phase, df, sloc="metadata_scala_code", stars="gh_stars", size="size_repo") {
  corpus_shrinking <<- add_row(
    corpus_shrinking, 
    phase=phase, 
    projects=nrow(df),
    sloc=ifelse(nrow(df) > 0, sum(df[, sloc], na.rm=T), 0),
    stars=ifelse(nrow(df) > 0, sum(df[, stars], na.rm=T), 0), 
    size=format_size(ifelse(nrow(df) > 0, sum(df[, size], na.rm=T), 0))
  )
}

add_phase("All projects", stage3_corpus)
add_phase("Metadata extraction",     filter(stage3_corpus, metadata_exit_code==0))
add_phase("Semanticdb generation",   filter(stage3_corpus, semanticdb_exit_code==0))
add_phase("Implicits extraction",    filter(stage3_corpus, implicits_exit_code==0))

corpus_shrinking %>% 
  my_datatable(colnames = c("Phase", "Number of projects", "Scala code", "GitHub Stars", "Project size"))
```

Connecting that with the data from stage 1:

```{r}
corpus_shrinking <- tibble(phase=character(0), projects=integer(0), sloc=integer(0), stars=integer(0), size=integer(0))

add_phase("Cloned projects", stage1_corpus, "scala_code")
add_phase("Compatible SBT projects", filter(stage1_corpus, compatible), "scala_code")
add_phase("Removed duplicate projects", anti_join(filter(stage1_corpus, compatible), stage3_corpus, by="project_id"), "scala_code")
add_phase("Without duplicates", stage3_corpus, "scala_code")
add_phase("Metadata extraction", filter(stage3_corpus, metadata_exit_code==0), size="size")
add_phase("Semanticdb generation", filter(stage3_corpus, semanticdb_exit_code==0), size="size")
add_phase("Implicits extraction", filter(stage3_corpus, implicits_exit_code==0), size="size")

corpus_shrinking %>% 
  my_datatable(colnames = c("Phase", "Number of projects", "Scala code", "GitHub Stars", "Project size"))
```

### Errors

The following analysis shall help to identify the error that occur in the different phases.

This table shows the summary of exit codes in the different phases:

```{r}
phases <- c("metadata", "compile", "semanticdb", "implicits")
phases_data <- list(
  select(stage3_corpus, project_id, exit_code=metadata_exit_code),
  select(stage3_corpus, project_id, exit_code=compile_exit_code),
  select(stage3_corpus, project_id, exit_code=semanticdb_exit_code),
  select(stage3_corpus, project_id, exit_code=implicits_exit_code)
)
status <- map2_dfr(phases, phases_data, ~phase_status(.x, .y)) %>% mutate_at(vars(-phase), function(x) coalesce(x, 0))
status %>% my_datatable()
```

#### Failed metadata, but has semanticdb

These are errors should be investigated closely, since extracting metadata should in general have less problems that generating semanticdb.
However, with SBT no one really knows.

```{r}
sdb_no_metadata <- filter(stage3_corpus, metadata_exit_code != 0, semanticdb_exit_code == 0) %>%
  select(project_id, metadata_exit_code, metadata_failure, metadata_failure_detail, metadata_duration, metadata_scala_code, gh_stars)
```

```{r}
sdb_no_metadata %>%
  count(metadata_failure) %>% 
  my_datatable()
```

```{r}
sdb_no_metadata %>% 
  my_datatable()
```

### Metadata and semanticdb errors

This is to guess what has happened.
Execution of each phase is stored in a log file.
The log files are stored in `<project_directory>/_analysis_/<phase>.log` where phase is one if (`compile`, `metadata`, `semanticdb`, `implcits`).
We query these log files for a number of regular expressions that identify common failures.
They are decribed in the `[guess_failure_cause](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/scripts/inc/functions.R#L181)` function.

```{r}
failed_projects <- filter(
  stage3_corpus, 
  metadata_exit_code == 1 | 
    (metadata_exit_code == 0 & semanticdb_exit_code == 1)
) %>%
  mutate(
    phase=case_when(
      metadata_exit_code == 1 ~ "metadata",
      semanticdb_exit_code == 1 ~ "semanticdb",
      TRUE ~ as.character(NA)
    ),
    cause=if_else(metadata_exit_code == 1, metadata_failure, semanticdb_failure),
    cause_detail=if_else(metadata_exit_code == 1, metadata_failure_detail, semanticdb_failure_detail),
    duration=if_else(phase=="metadata", metadata_duration, semanticdb_duration)
  ) %>%
  select(project_id, phase, cause, cause_detail, metadata_scala_code, gh_stars)
```

Summarized by exceptions:

```{r}
count(failed_projects, cause) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

```{r}
make_stats(
  add_nrow("Projects failed to build", filter(stage3_corpus, implicits_exit_code!=0 | is.na(metadata_scala_code))),
  add_nrow("Projects missing dependencies", filter(failed_projects, cause=="missing-dependencies")),
  add_nrow("Projects with compile error", filter(failed_projects, cause=="compilation-failed")),
  add_nrow("Projects with broken build", filter(failed_projects, cause=="project-loading-failed")),
  add_nrow("Projects with empty build", filter(stage3_corpus, implicits_exit_code==0, is.na(metadata_scala_code))),
  add_nrow("Projects missing Scala JS", filter(failed_projects, cause=="missing-dependencies", str_detect(cause_detail, "scalajs"))),
  add_nrow("Projects missing Snapshot Dependencies", filter(failed_projects, cause=="missing-dependencies", str_detect(cause_detail, "SNAPSHOT")))
) %>% my_datatable()
```


#### Missing dependencies

Missing dependencies is a common failure.
This section gives an overview which dependencies are missing to help to indentify if there are some patterns.
The `guess_failure_cause` function only considers the first missing dependency.

```{r}
count(filter(failed_projects, cause=="missing-dependencies"), cause_detail) %>% arrange(desc(n)) %>% my_datatable(colnames=c("Dependency", "Count"))
```

Notes:
- The `scalajs` is possibly missing because it has been removed due to some security vulnerabilities is some particular version.

#### List of Java errors

The list of JVM errors reported during any of the phases.
This helps us to identify if there are some patterns.

```{r}
filter(failed_projects, cause=="java-error") %>%
  mutate(
    log=make_corpus_link(path(project_id, "_analysis_", str_c(phase, ".log")))
  ) %>%
  select(-phase, -cause) %>%
  select(project_id, log, everything()) %>%
  my_datatable(escape=FALSE)
```


#### List of unknown problems

This is just to see of peraps there is something that could be done about it.
For our experience, these are simply broken builds.

```{r}
filter(failed_projects, cause=="unknown") %>%
  mutate(
    log=make_corpus_link(path(project_id, "_analysis_", str_c(phase, ".log")))
  ) %>%
  select(-phase, -cause) %>%
  select(project_id, log, everything()) %>%
  my_datatable(escape=FALSE)
```

## Missing Semanticdb

In some cases, the semanticdb would run, but not generate any output.
This is just a safety measure to list projects for which the pipeline reports a successful semanticdb generation, yet they need a manual inspection since no output has been generated.

```{r}
filter(corpus, semanticdb_exit_code==0, is.na(semanticdb_files) | semanticdb_files==0) %>%
  mutate(path=make_corpus_link(project_id)) %>%
  select(project_id, path, metadata_exit_code, scala_version, commit_count, gh_stars, metadata_scala_code) %>%
  my_datatable(escape=FALSE)
```

## Errors in implicit extraction

The final phase runs the [implicit extractor](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/libs/tools/src/main/scala/cz/cvut/fit/prl/scala/implicits/tools/ExtractImplicits.scala).
The extractor takes the generated semanticdb files together with the extracted metadata and build a [model](https://github.com/PRL-PRG/scala-implicits-analysis/blob/master/libs/model/src/main/protobuf/model.proto) of implicits.
Not all implcits that we find the semanticdb can be resolved and stored in our model.
There is a number of issues that can appear, but the main ones are:
- missing symbol - the implicit is referencing a symbol that cannot be found in neither the semanticdb nor the class path extracted from SBT
- unsupported type - semanticdb defines 14 different [Scala types](https://github.com/scalameta/scalameta/blob/master/semanticdb/semanticdb3/semanticdb3.md#scala-type). We do not support structural, dependent and existential types.
- missing term - the implicit call sites are injected by compiler. These are represented in semanticdb as AST [nodes](https://github.com/scalameta/scalameta/blob/master/semanticdb/semanticdb3/semanticdb3.md#tree) which are different from Scala AST. Our tool tries to map terms from one to the other, but it might not be possible every time. 

### Failures

This lists failures encountered when trying to extract implicits from semanticdb.
A failure is when the running the extractor did not terminate normally.

```{r}
filter(corpus, !is.na(implicit_failure)) %>%
  select(project_id, implicit_failure, metadata_scala_code, gh_stars) %>%
  my_datatable()
```

Notes:
- The `ZipException: error in opening zip file` means that it could not open a a dependency jar file.

### Errors

When extracting implicit declarations and call sites, there can be a number of problems.
Each of the gets logged into an ``r IMPLICITS_EXCEPTIONS`` files.
The following is their classification based on a number of regular expressions.

```{r load implicit errors}
exceptions <- read_feather(IMPLICIT_EXCEPTIOSN_F)
```

```{r}
classified_exceptions <- 
  mutate(
    exceptions,
    class=case_when(
      exception == "SymbolNotFoundException" & str_detect(message, "local\\d+") ~ "missing-local-symbol",
      exception == "SymbolNotFoundException" & str_detect(message, "symbol: local\\d+") ~ "missing-local-symbol",
      exception == "SymbolNotFoundException" & str_detect(message, "at Range\\(\\d+,\\d+,\\d+,\\d+\\)") ~ "missing-symbol-at-range",
      exception == "SymbolNotFoundException" ~ "missing-symbol-other",
      exception == "LoadingMetadataException" & str_detect(message, "No module found") & str_detect(message, "multi-jvm") ~ "missing-module-multijvm",
      exception == "LoadingMetadataException" & str_detect(message, "No module found") ~ "missing-module",
      exception == "LoadingMetadataException" ~ "metadata-loading",
      exception == "NotImplementedError" ~ "not-implemented",
      exception == "UnexpectedElementException" ~ "unexpected-element",
      exception == "UnsupportedElementException" ~ "unsupported-element",
      exception == "UnExtractableCallSiteException" & cause == "SymbolNotFoundException" ~ "unextractable-callsite-missing-term",
      exception == "UnExtractableCallSiteException" & cause == "FunctionNotFoundException" ~ "unextractable-callsite-missing-function",
      exception == "UnExtractableCallSiteException" ~ "unextractable-callsite-other",
      exception == "ImplicitArgumentNotFoundException" ~ "missing-implicit-arguments",
      exception == "Exception" & str_detect(message, "^Invalid call site argument") ~ "invalid-declaration",
      TRUE ~ "unclassified"
    )
  )
```

#### Per project summary

```{r}
count(classified_exceptions, project_id) %>% 
  left_join(select(corpus, project_id, metadata_scala_code, gh_stars), by="project_id") %>%
  arrange(desc(n)) %>% 
  my_datatable() 
```

#### Per error class summary

```{r}
count(classified_exceptions, class) %>% arrange(desc(n)) %>% my_datatable()
```

#### Per project and error class summary

```{r}
count(classified_exceptions, project_id, class) %>% arrange(desc(n)) %>% my_datatable()
```

#### Missing symbols

Missing local symbols (symbols defined just a block - we do not need to worry about them).

```{r}
filter(classified_exceptions, class=="missing-local-symbol") %>% 
  count(project_id) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

Missing symbols from a range when aligning the two ASTs.

```{r}
filter(classified_exceptions, class=="missing-symbol-at-range") %>% 
  count(project_id) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

All other missing symbols.

```{r}
filter(classified_exceptions, class=="missing-symbol-other") %>% 
  count(project_id) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

The following is a list of the missing symbols.
This might help us to identify is there are some patterns.

```{r}
filter(classified_exceptions, class=="missing-symbol-other") %>% 
  transmute(project_id, symbol=str_replace_all(message, ".*symbol: (.*)$", "\\1")) %>%
  count(project_id, symbol) %>% 
  arrange(desc(n)) %>% 
  my_datatable()
```

#### Unclassified

These are all the other errors that occured

```{r}
filter(classified_exceptions, class=="unclassified") %>%
  select(project_id, module_id, class, everything()) %>%
  my_datatable()
```

### Callsites and implicit callsites

A quick overview of the implicit call sites and implicit declarations.

```{r}
filter(corpus, implicits_exit_code==0, metadata_scala_code > 100) %>%
  mutate(
    outlier=is_outlier(callsites)|is_outlier(implicit_callsites),
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=callsites, 
      y=implicit_callsites, 
      label=label, 
      color=if_else(scaladex, "Library", "Application")
    )
  ) +
    geom_point(size=.5) +
    geom_text(size=3, check_overlap = T, vjust=1.5, na.rm = TRUE) + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    geom_vline(aes(xintercept=mean(callsites)), linetype=2, color="black", size=.2) + 
    geom_hline(aes(yintercept=mean(implicit_callsites)), linetype=2, color="black", size=.2) +
    labs(
      title="Ratio of regular call sites and implicit call sites",
      x="Number of callsites (log)", 
      y="Number of implicit callsites (log)", 
      color="Project type"
    )
```

```{r}
filter(corpus, implicits_exit_code==0, metadata_scala_code>0) %>%
  mutate(
    outlier=is_outlier(metadata_scala_code)|is_outlier(implicit_local_declarations),
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=metadata_scala_code, 
      y=implicit_local_declarations, 
      label=label, 
      color=if_else(scaladex, "Library", "Application")
    )
  ) +
    geom_point(size=.5) +
    geom_text(size=3, check_overlap = T, vjust=1.5, na.rm = TRUE) + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    geom_vline(aes(xintercept=mean(metadata_scala_code)), linetype=2, color="black", size=.2) + 
    geom_hline(aes(yintercept=mean(implicit_local_declarations)), linetype=2, color="black", size=.2) +
    labs(
      title="Ratio of regular source code size and number of implicit decalarations",
      x="Source lines of code (log)", 
      y="Number of local declarations (log)", 
      color="Project type"
    )
```

### Time of the successfull projects

Finally, we report the time each phase took for the successful projects.
This gives a hint about how long rebuilding the repository of passing projects will take.

```{r}
make_stats(
  add_num("Metadata extraction time", corpus$metadata_duration),
  add_num("Compile time", corpus$compile_duration),
  add_num("Semanticdb extraction time", corpus$semanticdb_duration),
  add_num("Implicits extraction time", corpus$implicits_duration)
) %>%
  my_datatable()
```
