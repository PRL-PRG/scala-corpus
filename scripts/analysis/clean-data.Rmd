---
title: "Cleanup the dataset from DejaVu"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pbapply)
library(tidyverse)
library(fs)
library(anytime)
```

# Configuration

```{r config}
N_JOBS <- 8

BASE_DIR <- "/mnt/array/scala-10.18"
DOWNLOAD_DIR <- path(BASE_DIR, "download")
DEJAVU_DIR <- path(BASE_DIR, "dejavu")
STARS_FILE <- path(BASE_DIR, "stars.csv")

ANALYSIS_DIRNAME <- "_analysis_"
REPO_METADATA_FILE <- path(ANALYSIS_DIRNAME, "repo-metadata.csv")
SLOC_FILE <- path(ANALYSIS_DIRNAME, "repo-sloc.csv")

ALL_SCALA_PROJECTS <- path(DEJAVU_DIR, "scala-projects-all.csv")
SBT_SCALA_PROJECTS <- path(DEJAVU_DIR, "scala-projects-sbt.csv")
```

# Load data

## Get all the projects

Find download and processed projects by dejavu

```{r list projects}
dirs <- dir_ls(DOWNLOAD_DIR, type="directory")
```

```{r projects data frame}
projects_paths_df <- data_frame(
  dejavu_id=basename(dirs) %>% as.integer(), 
  path=dirs
)
```

## Load repo metadata

```{r repo metadata}
repo_metadata_files <- path(dirs, REPO_METADATA_FILE)
```

Missing repo metadata files:
```{r}
repo_metadata_files_exists <- file_exists(repo_metadata_files)
missing_repo_metadata_files <- repo_metadata_files[!repo_metadata_files_exists]
repo_metadata_files <- repo_metadata_files[repo_metadata_files_exists]
```

```{r}
missing_repo_metadata_files
```

This will be the base of constracting a data frame containing all the projects
```{r}
repo_metadata_df <- map_dfr(
  repo_metadata_files, 
  ~read_csv(
    ., 
    col_types=cols(
      project_id="c",
      commit_count="i",
      commit="c",
      first_commit_date="i",
      commit_date="i",
      size="d"
    ), 
    col_names=c(
      "project_id", 
      "commit_count", 
      "commit", 
      "first_commit_date", 
      "commit_date", 
      "size"
    )
  )
)
```

### Convert commit times

```{r}
repo_metadata_df <- mutate(
  repo_metadata_df,
  first_commit_date=lubridate::as_datetime(first_commit_date),
  commit_date=lubridate::as_datetime(commit_date)
)
```

### Merge with prooject paths

```{r}
# getting the dejavu_id from ..../<id>/_analysis_/repo-metadata.csv (REPO_METADATA_FILE)
repo_metadata_files_ids <- as.integer(basename(dirname(dirname(repo_metadata_files))))
projects_df <- 
  projects_paths_df %>% 
  right_join(
    mutate(repo_metadata_df, dejavu_id=repo_metadata_files_ids),
    by="dejavu_id"
  )
```

Check for mistakes in the merging. This could potentially happen if there are some problems in the bulk read of the `REPO_METADATA_FILE` files.

```{r}
projects_df %>% filter(dejavu_id != as.integer(basename(path)))
```

## Remove the ones that point to the same origin

Now we need to filter out duplicates - these are caused by the errors in ghtorrent. The same github repository would be there more than once.

### Find duplicated projects based on their origins

- find projects that point to the same origin
- add commit informations to such duplicate projects

```{r}
duplicate_projects_df <- 
  group_by(projects_df, project_id) %>% 
  filter(n() > 1)
```

### Check if they have the same commits

- get the projects with multiple unique commits (muc)

```{r}
duplicate_projects_muc_df <- 
  group_by(duplicate_projects_df, project_id) %>% 
  summarise(unique_commits=length(unique(commit))) %>% 
  filter(unique_commits > 1)
```

- list the projects
```{r}
semi_join(duplicate_projects_df, duplicate_projects_muc_df, by="project_id") %>% 
  arrange(project_id)
```

### Partitioning the duplicates to ones we want to keep

We keep only the newest ones.

```{r}
duplicate_projects_keep_df <- 
  duplicate_projects_df %>% 
  group_by(project_id) %>% 
  filter(commit_date == max(commit_date)) %>% 
  distinct(project_id, .keep_all = TRUE)

duplicate_projects_remove_df <-
  anti_join(duplicate_projects_df, duplicate_projects_keep_df, by="dejavu_id")
```

```{r}
projects_all_df <- projects_df
```

```{r}
projects_df <- anti_join(projects_all_df, duplicate_projects_remove_df, by="dejavu_id")
```

## Load Github stars

```{r}
stars_df <- read_csv(STARS_FILE)
```

## Load build system

```{r}
get_build_system <- function(p) {
  systems_def <- c(
    "sbt"="build.sbt", 
    "maven"="pom.xml",
    "mill"="build.sc",
    "graddle"="build.gradle",
    "cbt"="build/build.scala",
    "maven-scala"="pom.scala",
    "maven-groovy"="pom.groovy",
    "maven-yml"="pom.yml",
    "maven-clojure"="pom.clojure",
    "maven-ruby"="pom.ruby",
    "maven-java"="pom.java",
    "make"="Makefile"
  )
  
  systems <- names(systems_def)
  system <- "unknown"

  for (i in seq_along(systems_def)) {
    if (file_exists(path(p, systems_def[i]))) {
      system <- systems[i]    
    }
  }
  
  system
}
```

```{r}
build_system_df <- 
  select(projects_df, dejavu_id, path) %>%
  mutate(
    build_system=pbapply::pbsapply(path, get_build_system, cl=N_JOBS)
  )
```


## Load SLOC

```{r}
projects_sloc_df <- projects_df %>% 
  rowwise() %>% 
  do(
    mutate(
      read_csv(
        path(.$path, SLOC_FILE), 
        col_types=cols(files="i", language="c", blank="i", comment="i", code="i"), 
        col_names=c("files", "language", "blank", "comment", "code")
      ), 
      dejavu_id=.$dejavu_id
    )
  )
```

### Summary for all languages

```{r}
sloc_summary_df <- projects_sloc_df %>%
  group_by(dejavu_id) %>%
  summarise(
    files=sum(files),
    sloc_code=sum(code),
    sloc_comment=sum(comment),
    sloc_blank=sum(blank),
    languages=str_c(language, collapse=",")
  )
```

### Summary for Scala only

```{r}
sloc_scala_summary_df <- projects_sloc_df %>%
  filter(language=="Scala") %>%
  group_by(dejavu_id) %>%
  summarise(
    scala_files=sum(files),
    scala_sloc_code=sum(code)
  )
```

## Merge

### Backup

```{r}
projects_backup_df <- projects_df
```

### Add stars

```{r}
projects_df <- inner_join(projects_df, select(stars_df, dejavu_id, stars), by="dejavu_id")
```

### Add build

```{r}
projects_df <- inner_join(projects_df, select(build_system_df, dejavu_id, build_system), by="dejavu_id")
```

### Add SLOC

```{r}
projects_df <- left_join(
  projects_df,
  sloc_summary_df,
  by="dejavu_id"
) %>% left_join(
  sloc_scala_summary_df,
  by="dejavu_id"
) %>% replace_na(
  list(
    files=0,
    sloc_code=0,
    sloc_comment=0,
    sloc_blank=0,
    languages="",
    scala_files=0,
    scala_sloc_code=0
  )
)
```

## Write all scala projects CSV file

```{r}
write_csv(projects_df, ALL_SCALA_PROJECTS)
```

## SBT projects

```{r}
sbt_projects_df <- filter(projects_df, build_system=="sbt")
```

```{r}
write_csv(sbt_projects_df, SBT_SCALA_PROJECTS)
```

## Removing projects that have all files duplicated

Load the files hashes from dejavu.
```{r}
files_hashes <- read_csv(
  # norally the file is in dataset/scala, but Peta had to rerun the dejavu due to the bug of skipping some files
  path(DEJAVU_HOME, "files.csv.h2i"), 
  col_names=c("file_id", "project_id", "relative_url", "file_hash"), 
  col_types=cols(file_id="i", project_id="i", relative_url="c", file_hash="i")
)
```

We only consider the files that are in the sbt projects
```{r}
files_considered <- semi_join(files_hashes, projects_sbt, by="project_id")
```


Compute the level of duplication
```{r}
files_duplicates <- count(files_considered, file_hash) %>% mutate(duplicated=n > 1)
files_project_duplicates <- left_join(files_considered, select(files_duplicates, -n), by="file_hash")
files_project_duplication <- group_by(files_project_duplicates, project_id) %>% summarise(duplication=sum(duplicated)/n())
```


Look at the missing projects - dejavu does not always process all files.
Most of the missing ones shall be empty - no `.scala` files, but we need to check which are the ones that it skipped:
```{r}
projects_missing <- anti_join(projects_sbt, files_project_duplication, by="project_id")

find_scala_files <- Vectorize(function(dir) {
   length(dir_ls(dir, glob="*.scala", recursive=TRUE))
}, USE.NAMES = FALSE)

projects_missing_n_scala <- projects_missing %>% mutate(scala_files=find_scala_files(dir))

filter(projects_missing_n_scala, scala_files > 0)
```


File level duplication in the sbt projects
```{r}
files_project_duplication %>% 
  ggplot(aes(x=duplication)) + 
  geom_histogram(binwidth = .05) +
  labs(x="Duplication (num of duplicate files / num of all files)", y="Number of projects", title="Files duplication in Scala") +
  scale_x_continuous(labels=scales::percent_format()) +
  scale_y_continuous(labels=scales::comma_format())
```

```{r}
projects_sbt_duplication <- projects_sbt %>% right_join(files_project_duplication, by="project_id")
```

Keep only the projects that have less than 75%
```{r}
projects_final <- projects_sbt %>% semi_join(filter(files_project_duplication, duplication < 1), by="project_id")
```






Store actual scala-projects CSV:
```{r}
write_csv(projects_sbt_duplication, "../scala-projects.csv")
```

```{r}
projects_sbt_duplication$project %>% 
  writeLines("../projects-all.txt")
```




```{r}
commits <- pbsapply(dirs, git_last_commit, cl=N_JOBS)
first_commit_time <- pbsapply(dirs, git_first_commit_time, cl=N_JOBS)
last_commits_time <- pbsapply(dirs, git_last_commit_time, cl=N_JOBS)
slocs <- pbsapply(dirs, get_sloc, cl=N_JOBS)
```

```{r}
raw_sizes <- pbsapply(dirs, get_size, cl=N_JOBS)
```

```{r}
projects <- data_frame(
  project_id=str_replace(dirs, ".*/tmp/([0-9]+)", "\\1") %>% as.integer(), 
  project=str_replace(origins, "http://github.com/([^/]+)/(.*).git$", "\\1--\\2"),
  dir=dirs, 
  origin=origins,
  commit=commits, 
  first_commit=as.integer(first_commits),
  last_commit=as.integer(last_commits)
)
```

## Load file-level information from DejaVu


# Remove duplicated projects based on origins


Final projects to keep
```{r}
projects_final <- projects %>% anti_join(duplicate_projects_remove, by="project_id")
```


## Find the unique file

```{r}
files_kept <- inner_join(files, projects_final, by="project_id")
```


## Remove duplicate projects

Removing the `r nrow(duplicate_projects_remove)` duplicate project will remove `r nrow(files) - nrow(files_kept)`.

### Remove dirs
```{r}
dirs_to_remove <- duplicate_projects_remove$dir
dir_delete(dirs_to_remove[dir_exists(dirs_to_remove)])
```

## Create unique file lists

If there are multiple equivalent files, take the one from from an older project.
```{r}
unique_files <- 
  group_by(files_kept, file_hash) %>%
  filter(first_commit == min(first_commit)) %>%
  ungroup()
```

There are projects which are effictivelly forks, but have not been created as proper forks.
```{r}
bad_forks <-
  group_by(unique_files, file_hash) %>% filter(n() > 1, length(unique(project_id)) > 1)
```

```{r}
all <- inner_join(unique_files, projects_final, by="project_id")
```

```{r}
write_csv(all, path(RUN_HOME, "unique-files.csv"))
transmute(all, path=path(dir, relative_url), path_rel=str_replace(path, ".*/(ghtorrent/.*)", "\\1")) %>%
  .$path_rel %>%
  write_lines(path(RUN_HOME, "unique-files.txt"))
```

```{r}
save_one <- function(df) {
    analysis_dir <- path(df$dir[1], "_analysis_")
    dir_create(analysis_dir)
    
    write_csv(data_frame(project=df$project[1], github_url=df$origin[1], commit=df$commit[1], timestamp=df$timestamp[1]), "metadata.csv")
    write_csv(select(df, project, project_id, path=relative_url), path(analysis_dir, "unique-files.csv"))
    write_lines(df$relative_url, path(analysis_dir, "unique-files.txt"))
    
    data_frame()
}

all %>% group_by(project_id) %>% do(save_one(.))
```

## Project mapping between ghtorrent and names

```{r}
mapping <- transmute(projects_final, project, project_id, dir=path("ghtorrent/tmp", project_id), link=path("projects", project))
write_csv(mapping, path(RUN_HOME, "ghtorrent-link-mappings.csv"))
write_lines(mapping$project, path(RUN_HOME, "projects.txt"))

link_src <- mapping$dir
link_trg <- mapping$link
link_src <- link_src[!file_exists(link_trg)]
link_trg <- link_trg[!file_exists(link_trg)]

write_lines(str_c("ln -s ../", link_src, " ", str_replace(link_trg, "projects/", "")), path(RUN_HOME, "make-links.sh"))
```

## Partition the files to 2 disks

```{r}
find_partition <- function(xs) {
    A <- numeric()
    A_sum <- 0
    B <- numeric()
    B_sum <- 0
    
    xs <- sort(xs, dec=TRUE)
    
    for (i in seq_along(xs)) {
        if (A_sum < B_sum) {
           A <- append(A, i)
           A_sum <- A_sum + xs[i]
        } else {
           B <- append(B, i)
           B_sum <- B_sum + xs[i]
        }
    }
}
```

