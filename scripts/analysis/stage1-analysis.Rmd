---
title: "Corpus analysis"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
params:
  base_dir: /var/lib/scala/corpora/github
  base_url: http://prl1.ele.fit.cvut.cz:8149
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fs)
library(tidyverse)
library(ggplot2)
library(DT)
library(lubridate)
library(knitr)
library(glue)
library(pbapply)

theme_set(theme_minimal())

options(corpus_dir=params$base_dir)

source("../inc/paths.R")
source("../inc/functions.R")

pboptions(type="txt")
```

The result of this analysis should be a file called `r PROJECTS_FILE` which is the list of project ids that should go to the next stage.

## Loading data

```{r loading data}
corpus <- read_csv(CORPUS_STAGE1, col_types=cols(
  project_id = col_character(),
  compatible = col_logical(),
  origin = col_character(),
  build_system = col_character(),
  sbt_version = col_character(),
  size = col_integer(),
  commit_count = col_integer(),
  commit = col_character(),
  commit_date = col_datetime(format = ""),
  first_commit_date = col_datetime(format = ""),
  scala_code = col_integer(),
  scala_files = col_integer(),
  dejavu_n_files = col_integer(),
  dejavu_n_duplicated_files = col_integer(),
  dejavu_duplication = col_double(),
  gh_name = col_character(),
  gh_stars = col_integer(),
  gh_watchers = col_integer(),
  gh_created_at = col_datetime(format = ""),
  gh_updated_at = col_datetime(format = ""),
  gh_pushed_at = col_datetime(format = ""),
  gh_fork = col_logical(),
  gh_archived = col_logical(),
  scaladex = col_logical()
))
```

The data in the `r CORPUS_STAGE1` has the following meaning:

- `project_id` (chr): project name as `github-user-name--github-repository-name`
- `compatible` (lgl): has some Scala code and uses compatible SBT (>= 0.13.5+ or >= 1.0.0)
- `origin` (chr): the URL to github 
- `build_system` (chr): guessed build system
- `sbt_version` (chr):  guessed sbt version
- `size` (int): of the repository in bytes (in some projects it includes class files)
- `commit_count` (int): number of commits
- `commit` (chr): current checkout hash
- `commit_date` (dttm): the date of the current commit
- `first_commit_date` (dttm): the date of the first commit
- `scala_code` (int): number of lines of Scala code excluding blanks and comments
- `scala_files` (int): number of Scala files
- `dejavu_n_files` (int): number of files processed by Dejavu (this shall be less than `scala_files` since it only consider non-token-empty ones with `.scala suffix`)
- `dejavu_n_duplicated_files` (int):  number of duplicate files (files seen in other projects)
- `dejavu_duplication` (dbl): `dejavu_n_duplicated_files/dejavu_n_files`
- `gh_name` (chr): GitHub name
- `gh_stars` (int): Number of stars 
- `gh_watchers` (int): Number of watchers
- `gh_created_at` (dttm): When was the project created at GitHub
- `gh_updated_at` (dttm): When was the project last updated at GitHub
- `gh_pushed_at` (dttm): When it has been last pushed to the project at GitHub
- `gh_fork` (lgl): Is it a fork?
- `gh_archived` (lgl): Has it been archived?
- `scaladex` (lgl): Has it been indexed by scaladex?

## Checks for duplicits

Because of [#53](https://github.com/PRL-PRG/scala-implicits-analysis/issues/53), we could run into duplicates.
This is to check if we do or not

```{r checking duplicates}
stopifnot(sum(duplicated(tolower(corpus$project_id)))==0)
```

## Basics

This corpus has been created as a merge between 

- all Scala projects from [ghtorrent](http://ghtorrent.org/) from [January 2019](http://ghtorrent-downloads.ewi.tudelft.nl/mysql/mysql-2019-01-01.tar.gz),
- and all indexed Scala projects generated by querying [scaladex index](http://index.scala-lang.org/) in January 2019.

The way the process work is that:

- Stage 1:

    1. create `all-projects.txt` that contains a new-line separated list of project_id (`<github-user-name>--<github-repo-name>`) (`status=0`),
    2. download projects into `all-projects` folder and optionally patch them with information in `all-projects-patches.csv` (`status=1`),
    3. extract `repo-metadata.csv` that contains basic metadata about the repository including scala lines of code, build system and optionally SBT version,
    4. filter non-empty and SBT compatible projects (projects using SBT >= 0.13.5 or 1.0) (`status=2`),
    5. fetch information from GitHub about all the SBT projects
    6. run dejavu
    7. create `corpus-stage1.csv` summary
    8. run the `stage1-analysis.Rmd` to produce the `projects.txt` file
  
- Stage 2:

    9. compile all projects
    10. extract SBT metadata for each project
    11. extract semanticdb for each project
  
- Stage3:

    10. extract implicits for each project
    10. merge all the results
    11. export implicits into CSV files
    12. create `corpus-stage3.csv` summary
    13. run the `stage3-analysis.Rmd`

All these tasks are run from a `Makefile.corpus` makefile.

## Stage 1 Corpus Summary

```{r}
sbt_projects <- 
  filter(corpus, build_system=="sbt")

projects <- 
  filter(corpus, compatible)
```

```{r basic stats}
make_stats(
  add_num("All projects SLOC", corpus$scala_code),
  add_num("All projects STARS", corpus$gh_stars),
  add_num("SBT projects SLOC", sbt_projects$scala_code),
  add_num("SBT projects STARS", sbt_projects$gh_stars),
  add_num("Compatible SBT projects SLOC", projects$scala_code),
  add_num("Compatible SBT projects STARS", projects$gh_stars)
) %>% 
  my_datatable()
```

### Overview of the build systems

We only support SBT >= 0.13.5 | 1.0.0 because we need auto plugins feature.
Breakdown on what are the other build systems used across the projects.

```{r build system overview}
group_by(corpus, build_system) %>% 
  summarise(count=n(), scala_code=sum(scala_code, na.rm=T), gh_stars=sum(gh_stars, na.rm=T)) %>%
  replace_na(list(build_system="unknown")) %>% 
  arrange(desc(count)) %>% 
  my_datatable(colnames = c("Build system", "Count", "Scala Code", "Github Stars"))
```

### The _unknown_ build system?

This is here just to understand what are the largest of the projects for which we cannot guess build system.

```{r top100 unknown build system}
filter(corpus, is.na(build_system)) %>% 
  top_n(100, scala_code) %>%
  arrange(desc(scala_code)) %>% 
  mutate(project_id=make_link(origin, project_id), size=format_size(size)) %>%
  select(project_id, scala_code, commit_count, size, commit_date) %>%
  my_datatable(escape=FALSE)
```

### Overview of the sbt versions

Breakdown on the SBT versions.

#### Major versions

```{r sbt major versions}
filter(corpus, build_system=="sbt") %>%
  mutate(
    sbt_version=parse_sbt_version(sbt_version),
    sbt_version=case_when(
       map_lgl(sbt_version, ~all(. >= c(1,0,0))) ~ ">= 1.0",
       map_lgl(sbt_version, ~all(. >= c(0,13,5))) ~ ">= 0.13.5",
       map_lgl(sbt_version, ~(all(.[2:3] < c(13,5) & .[1] == 0))) ~ "< 0.13.5",
      TRUE ~ "Unknown"
    )
  ) %>%
  count(sbt_version) %>% 
  arrange(desc(n)) %>%
  my_datatable(colnames = c("SBT version", "Count"))
```

#### All versions

```{r sbt all versions}
filter(projects, build_system=="sbt") %>% 
  count(sbt_version) %>% 
  replace_na(list(sbt_version="Not specified")) %>% 
  arrange(desc(n)) %>%
  my_datatable(page_size=10, colnames = c("SBT version", "Count"))
```

#### Missing sbt.version

There are projects that do not define which SBT version that use.
We skip them since most of them are not worth the trouble.

```{r missing sbt.version}
missing_sbt_version <- 
  filter(sbt_projects, build_system == "sbt", is.na(sbt_version)) %>%
  mutate(
    sbt_properties=path(params$base_dir, "all-projects", project_id, "project", "build.properties"), 
    sbt_properties=ifelse(file_exists(sbt_properties), make_corpus_link(sbt_properties), NA)
  ) %>%
  select(project_id, commit_count, scala_code, gh_stars, sbt_properties)
```

The following projects do not know that it is `sbt.version` and use
`bt.version` or `sbtversion` :-)

```{r misspelled sbt.version}
filter(missing_sbt_version, !is.na(sbt_properties)) %>%
  my_datatable(page_size=10, escape = FALSE)
```

The following projects do not care, as they do not define `build.properties`:
```{r missing build.properties}
filter(missing_sbt_version, is.na(sbt_properties)) %>%
  select(-sbt_properties) %>%
  mutate(path=make_corpus_link(project_id)) %>%
  my_datatable(page_size=10, escape = FALSE)
```

## Duplicates from Dejavu

```{r}
dejavu_duplication_summary <- 
  tibble(duplication=1:100/100) %>%
  mutate(
    nprojects=map_int(duplication, ~nrow(filter(projects, dejavu_duplication >= .))),
    scala_code=map_int(duplication, ~sum(filter(projects, dejavu_duplication >= .)$scala_code)),
    gh_stars=map_int(duplication, ~sum(filter(projects, dejavu_duplication >= .)$gh_stars))
  )
```

The following is a summary of dejavu duplicates. It says how many projects (and corresponding Scala code and GitHub stars) have at least X ammount of file-level duplication.
The red line is the current treshold after which we filter by the number of GitHub stars as well.

```{r dejavu duplication plot}
dejavu_duplication_summary %>%
  gather("key", "value", nprojects, scala_code, gh_stars) %>%
  ggplot(aes(x=duplication, y=value)) +
    geom_bar(stat='identity') +
    facet_grid(rows=vars(key), scales="free_y") +
    scale_x_continuous(labels=scales::percent) +
    scale_y_continuous(labels=scales::comma) +
    geom_vline(aes(xintercept=params$duplication_treshold), linetype=2, color="red", size=0.2) + 
    labs(title="Dejavu duplication > 0", subtitle="Sum in projects that have at least that duplication", x="Duplication", y="Sum")
```

The following is a simple histogram of all the duplication.

```{r histogram of dejavu duplication}
filter(projects, dejavu_duplication > 0) %>%
  ggplot(aes(dejavu_duplication)) +
  geom_histogram(bins=100) +
  scale_x_continuous(labels=scales::percent) +
  scale_y_continuous(labels=scales::comma) +
  labs(title="Dejavu duplication > 0", subtitle="Number of duplicated files / number of files", x="Duplication", y="Number of projects")
```

### A table version

```{r dejavu duplication summary}
dejavu_duplication_summary %>%
  my_datatable(page_size=10)
```

## Stage 2 Corpus 

```{r}
treshold_duplication<- .75
treshold_lifespan_months <- 3
treshold_gh_stars <- 5
treshold_commit_count <- 2
```

The final set of projects that will go to the next stage is composed based on the following criteria:

- at least `r treshold_commit_count` commits,
- at least `r treshold_lifespan_months` months between project creation at github and the last push,
- projects with more than `r ptreshold_duplication*100`% duplication, at least `r treshold_gh_stars` stars on GitHub.

```{r}
lifespan_treshold <- months(treshold_lifespan_months)
stage2_projects <- 
  filter(projects, 
         commit_count >= treshold_commit_count,
         gh_pushed_at-gh_created_at > lifespan_treshold,
         dejavu_duplication < treshold_duplication | gh_stars >= treshold_gh_stars
  )
```

The `project_id` will be inserted into `r PROJECTS_FILE` for the next stage.

```{r stage2 project.txt creation}
write_lines(stage2_projects$project_id, PROJECTS_FILE)
```

### Summary

```{r stage2 overview}
make_stats(
  add_num("Stage2 Project SLOC", stage2_projects$scala_code),
  add_num("Stage2 Project FILES", stage2_projects$scala_files),
  add_num("Stage2 Project STARS", stage2_projects$gh_stars),
  add_num("Stage2 Project COMMITS", stage2_projects$commit_count),
  add_num("Stage2 Project LIFESPAN (days)", as.numeric(stage2_projects$gh_pushed_at-stage2_projects$gh_created_at, unit="days")),
  add_num("Stage2 Project Duplication", stage2_projects$dejavu_duplication)
) %>%
  my_datatable()
```

### How much Spark do we have

It seems that people like to have their own version of Spark.
The problem is that Spark is big (the biggest Scala project excluding the JS stuff) so it will potetially skew the data.
In the corpus we have:

```{r corpus spark overview}
spark <-corpus %>%
  filter(
    str_detect(tolower(project_id), "spark"), 
    scala_code > 1e5 # spark has over 100K SLOC
  )

make_stats(
  add_num("Ratio of Spark projects in Corpus SLOC", sum(spark$scala_code)/sum(corpus$scala_code, na.rm = T)),
  add_num("Spark projects in Corpus SLOC", spark$scala_code),
  add_num("Spark projects in Corpus STARS", spark$gh_stars)
) %>%
  my_datatable()
```

Who are they:

```{r}
spark %>% 
  select(project_id, origin, commit_count, scala_code, gh_stars, dejavu_duplication, gh_created_at, gh_pushed_at, scaladex) %>%
  my_datatable(page_size=10)
```

After filtering we get

```{r stage2 spark overview}
spark <- stage2_projects %>%
  filter(str_detect(tolower(project_id), "spark"), scala_code > 1e5)

make_stats(
  add_num("Ratio of Spark projects in Corpus SLOC", sum(spark$scala_code)/sum(stage2_projects$scala_code, na.rm = T)),
  add_num("Spark projects SLOC", spark$scala_code),
  add_num("Spark projects STARS", spark$gh_stars)
) %>%
  my_datatable()
```

Who are they:

```{r}
spark %>% 
  select(project_id, origin, commit_count, scala_code, gh_stars, dejavu_duplication, gh_created_at, gh_pushed_at, scaladex) %>%
  my_datatable(page_size=10)
```

### Visualization

```{r stage2 project visualization}
stage2_projects %>%
  mutate(
    outlier=is_outlier(scala_code)|is_outlier(commit_count)|is_outlier(gh_stars),
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=scala_code, 
      y=commit_count, 
      label=label, 
      color=if_else(scaladex, "Library", "Application"),
      size=gh_stars
    )
  ) +
    geom_point() +
    geom_text(size=3, check_overlap = T, vjust=1.5, na.rm = TRUE) + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    scale_size_continuous(range = c(.1, 5)) +
    geom_hline(aes(yintercept=mean(commit_count)), linetype=2, color="black", size=0.2) +
    geom_vline(aes(xintercept=mean(scala_code)), linetype=2, color="black", size=0.2) + 
    labs(
      x="Souce lines of code (log)", 
      y="Number of commits (log)", 
      color="Project type",
      size="Github stars"
    )
```

### Top projects

The TOP50 by Scala code, commit count and stars.

```{r stage2 top projects}
bind_rows(
  top_n(stage2_projects, 50, scala_code),
  top_n(stage2_projects, 50, commit_count),
  top_n(stage2_projects, 50, gh_stars)
) %>%
  distinct(project_id, .keep_all = TRUE) %>%
  my_datatable(page_size=10)
```