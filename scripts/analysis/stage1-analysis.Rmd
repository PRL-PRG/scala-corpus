---
title: "Stage 1 Analysis"
authors: Filip Krikava, Jan Vitek and Heather Miller
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
params:
  base_dir: ../../../corpora/4-github
  lib_dir: ../inc
  base_url: http://prl1.ele.fit.cvut.cz:8149
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(glue)
library(ggplot2)
library(knitr)
library(fs)
library(DT)
library(pbapply)
library(feather)

theme_set(theme_minimal())

options(corpus_dir=params$base_dir)

source(path(params$lib_dir, "paths.R"))
source(path(params$lib_dir, "functions.R"))

pboptions(type="txt")
```

## Overview

The goal of the stage 1 analysis is to figure out which projects from the initial set of projects are eligible for the next stage, i.e. for compilation and data extraction.

The `r CORPUS_STAGE1_F` file has been created by the [corpus-stage1.R](https://github.com/PRL-PRG/scala-implicits-analysis/blob/master/scripts/corpus-stage1.R) which essentially merges data from a number of files:

- `r ALL_PROJECTS_FILE`
- `r SBT_PROJECTS_FILE`
- `r DEJAVU_DUPLICATION`
- `r GITHUB_INFO`
- `r REPO_METADATA`
- `r SCALADEX`

It contains the folling data:

- `project_id` (chr): project name as `github-user-name--github-repository-name`
- `compatible` (lgl): has some Scala code and uses compatible SBT (>= 0.13.5+ or >= 1.0.0)
- `origin` (chr): the URL to github 
- `build_system` (chr): guessed build system
- `sbt_version` (chr):  guessed sbt version
- `size_repo` (int): of the git versioned files in bytes
- `size` (int): of everything in bytes
- `commit_count` (int): number of commits
- `commit` (chr): current checkout hash
- `commit_date` (dttm): the date of the current commit
- `first_commit_date` (dttm): the date of the first commit
- `scala_code` (int): number of lines of Scala code excluding blanks and comments
- `scala_files` (int): number of Scala files
- `dejavu_n_files` (int): number of files processed by Dejavu (this shall be less than `scala_files` since it only consider non-token-empty ones with `.scala suffix`)
- `dejavu_n_duplicated_files` (int):  number of duplicate files (files seen in other projects)
- `dejavu_duplication` (dbl): `dejavu_n_duplicated_files/dejavu_n_files`
- `gh_name` (chr): GitHub name
- `gh_stars` (int): Number of stars 
- `gh_watchers` (int): Number of watchers
- `gh_created_at` (dttm): When was the project created at GitHub
- `gh_updated_at` (dttm): When was the project last updated at GitHub
- `gh_pushed_at` (dttm): When it has been last pushed to the project at GitHub
- `gh_fork` (lgl): Is it a fork?
- `gh_archived` (lgl): Has it been archived?
- `scaladex` (lgl): Has it been indexed by scaladex?

## Loading data

```{r loading data}
corpus <- read_feather(CORPUS_STAGE1_F) %>% filter(scala_code > 0)
```

### Checks for duplicits

Because of [#53](https://github.com/PRL-PRG/scala-implicits-analysis/issues/53), we could run into duplicates.
This is to check if we do or not

```{r checking duplicates}
stopifnot(sum(duplicated(tolower(corpus$project_id)))==0)
```

## Stage 1 Corpus Summary

```{r}
sbt_projects <- 
  filter(corpus, build_system=="sbt")

projects <- 
  filter(corpus, compatible)
```

```{r basic stats}
make_stats(
  add_nrow("All Projects", corpus),
  add_num("All Projects code", corpus$scala_code),
  add_num("All Projects stars", corpus$gh_stars),
  add_nrow("SBT projects", sbt_projects),
  add_num("SBT projects code", sbt_projects$scala_code),
  add_num("SBT projects stars", sbt_projects$gh_stars),
  add_nrow("Compatible SBT projects", projects),
  add_num("Compatible SBT projects code", projects$scala_code),
  add_num("Compatible SBT projects stars", projects$gh_stars)
) %>% 
  my_datatable()
```

### Overview of the build systems

We only support SBT >= 0.13.5 | 1.0.0 because we need auto plugins feature.
Breakdown on what are the other build systems used across the projects.

```{r build system overview}
group_by(corpus, build_system) %>% 
  summarise(count=n(), scala_code=sum(scala_code, na.rm=T), gh_stars=sum(gh_stars, na.rm=T)) %>%
  replace_na(list(build_system="unknown")) %>% 
  arrange(desc(count)) %>% 
  my_datatable(colnames = c("Build system", "Count", "Scala Code", "Github Stars"))
```

```{r}
make_stats(
  add_nrow("Maven Projects", filter(corpus, build_system=="maven")),
  add_nrow("Graddle Projects", filter(corpus, build_system=="graddle")),
  add_nrow("Unknown Build Systen", filter(corpus, is.na(build_system)))
)
```


### The _unknown_ build system?

This is here just to understand what are the largest of the projects for which we cannot guess build system.

```{r top100 unknown build system}
filter(corpus, is.na(build_system)) %>% 
  top_n(100, scala_code) %>%
  arrange(desc(scala_code)) %>% 
  mutate(project_id=make_link(origin, project_id), size=format_size(size)) %>%
  select(project_id, scala_code, commit_count, size, commit_date) %>%
  my_datatable(escape=FALSE)
```

### Overview of the sbt versions

Breakdown on the SBT versions.

#### Major versions

```{r sbt major versions}
filter(corpus, build_system=="sbt") %>%
  mutate(
    sbt_version=parse_sbt_version(sbt_version),
    sbt_version=case_when(
       map_lgl(sbt_version, ~all(. >= c(1,0,0))) ~ ">= 1.0",
       map_lgl(sbt_version, ~all(. >= c(0,13,5))) ~ ">= 0.13.5",
       map_lgl(sbt_version, ~(all(.[2:3] < c(13,5) & .[1] == 0))) ~ "< 0.13.5",
      TRUE ~ "Unknown"
    )
  ) %>%
  count(sbt_version) %>% 
  arrange(desc(n)) %>%
  my_datatable(colnames = c("SBT version", "Count"))
```

#### All versions

```{r sbt all versions}
filter(projects, build_system=="sbt") %>% 
  count(sbt_version) %>% 
  replace_na(list(sbt_version="Not specified")) %>% 
  arrange(desc(n)) %>%
  my_datatable(page_size=10, colnames = c("SBT version", "Count"))
```

#### Missing sbt.version

There are projects that do not define which SBT version that use.
We skip them since most of them are not worth the trouble.

```{r missing sbt.version}
missing_sbt_version <- 
  filter(sbt_projects, build_system == "sbt", is.na(sbt_version)) %>%
  mutate(
    sbt_properties=path(params$base_dir, "all-projects", project_id, "project", "build.properties"), 
    sbt_properties=ifelse(file_exists(sbt_properties), make_corpus_link(sbt_properties), NA)
  ) %>%
  select(project_id, commit_count, scala_code, gh_stars, sbt_properties)
```

The following projects do not know that it is `sbt.version` and use
`bt.version` or `sbtversion`.

```{r misspelled sbt.version}
filter(missing_sbt_version, !is.na(sbt_properties)) %>%
  my_datatable(page_size=10, escape = FALSE)
```

The following is the same of projects that do not care, as they do not define `build.properties`:
```{r missing build.properties}
filter(missing_sbt_version, is.na(sbt_properties)) %>%
  select(-sbt_properties) %>%
  top_n(100, scala_code) %>%
  mutate(path=make_corpus_link(project_id)) %>%
  my_datatable(page_size=10, escape = FALSE)
```

## Duplicates from Dejavu

```{r}
dejavu_duplication_summary <- 
  tibble(duplication=1:100/100) %>%
  mutate(
    nprojects=map_int(duplication, ~as.integer(nrow(filter(projects, dejavu_duplication >= .)))),
    scala_code=map_int(duplication, ~as.integer(sum(filter(projects, dejavu_duplication >= .)$scala_code))),
    gh_stars=map_int(duplication, ~as.integer(sum(filter(projects, dejavu_duplication >= .)$gh_stars)))
  )
```

The following is a summary of dejavu duplicates. It says how many projects (and corresponding Scala code and GitHub stars) have at least X ammount of file-level duplication.
The red line is the current treshold after which we filter by the number of GitHub stars as well.

```{r dejavu duplication plot}
dejavu_duplication_summary %>%
  gather("key", "value", nprojects, scala_code, gh_stars) %>%
  ggplot(aes(x=duplication, y=value)) +
    geom_bar(stat='identity') +
    facet_grid(rows=vars(key), scales="free_y") +
    scale_x_continuous(labels=scales::percent) +
    scale_y_continuous(labels=scales::comma) +
    geom_vline(aes(xintercept=.75), linetype=2, color="red", size=0.2) + 
    labs(title="Dejavu duplication > 0", subtitle="Sum in projects that have at least that ammount of duplication", x="Duplication", y="Sum")
```

The following is a simple histogram of all the duplication.

```{r histogram of dejavu duplication}
filter(projects, dejavu_duplication > 0) %>%
  ggplot(aes(dejavu_duplication)) +
  geom_histogram(bins=100) +
  scale_x_continuous(labels=scales::percent) +
  scale_y_continuous(labels=scales::comma) +
  labs(title="Dejavu duplication > 0", subtitle="Number of duplicated files / number of files", x="Duplication", y="Number of projects")
```

### A table version

```{r dejavu duplication summary}
dejavu_duplication_summary %>%
  my_datatable(page_size=10)
```

## Stage 2 Corpus 

Any changes to the following variables must be then replicated in the `corpus-stage1.R`.
Here it is to experiment wirh.

```{r}
treshold_duplication<- .75
treshold_lifespan_months <- 3
treshold_gh_stars <- 5
treshold_commit_count <- 2
```

The final set of projects that will go to the next stage is composed based on the following criteria:

- at least `r treshold_commit_count` commits,
- at least `r treshold_lifespan_months` months between project creation at github and the last push,
- projects with more than `r treshold_duplication*100`% duplication, at least `r treshold_gh_stars` stars on GitHub.

```{r}
lifespan_treshold <- months(treshold_lifespan_months)
stage2_projects <- 
  filter(projects, 
         commit_count >= treshold_commit_count,
         scaladex | (gh_pushed_at-gh_created_at > lifespan_treshold),
         dejavu_duplication < treshold_duplication | gh_stars >= treshold_gh_stars
  )
```

### Summary

Based on the threshold, these are the projects that can go to stage 2.

```{r stage2 overview}
make_stats(
  add_nrow("Analyzable Projects", stage2_projects),
  add_num("Analyzable Projects code", stage2_projects$scala_code),
  add_num("Analyzable Projects diles", stage2_projects$scala_files),
  add_num("Analyzable Projects stars", stage2_projects$gh_stars),
  add_num("Analyzable Projects commits", stage2_projects$commit_count),
  add_percent("Analyzable Projects duplication", mean(stage2_projects$dejavu_duplication))
) %>%
  my_datatable()
```

```{r stage2 project visualization}
stage2_projects %>%
  mutate(
    outlier=is_outlier(scala_code)|is_outlier(commit_count)|is_outlier(gh_stars),
    label=if_else(outlier, str_glue("{project_id}"), as.character(NA))
  ) %>%
  ggplot(
    aes(
      x=scala_code, 
      y=commit_count, 
      label=label, 
      color=if_else(scaladex, "Library", "Application"),
      size=gh_stars
    )
  ) +
    geom_point() +
    geom_text(size=3, check_overlap = T, vjust=1.5, na.rm = TRUE) + 
    scale_x_log10(labels = scales::comma) + 
    scale_y_log10(labels = scales::comma) +
    scale_size_continuous(range = c(.1, 5)) +
    geom_hline(aes(yintercept=mean(commit_count)), linetype=2, color="black", size=0.2) +
    geom_vline(aes(xintercept=mean(scala_code)), linetype=2, color="black", size=0.2) + 
    labs(
      x="Souce lines of code (log)", 
      y="Number of commits (log)", 
      color="Project type",
      size="Github stars"
    )
```

### How much Spark do we have

It seems that people like to have their own version of Spark.
The problem is that Spark is big (the biggest Scala project excluding the JS stuff) so it will potetially skew the data.
In the corpus we have:

```{r corpus spark overview}
spark <- corpus %>%
  filter(
    str_detect(tolower(project_id), "spark"), 
    scala_code > 1e5 # spark has over 100K SLOC
  )

make_stats(
  add_nrow("Spark projects (in corpus)", spark),
  add_num("Spark code (in corpus)", spark$scala_code),
  add_percent("Ratio of Spark code (in corpus)", sum(spark$scala_code)/sum(corpus$scala_code, na.rm=T))
) %>%
  my_datatable()
```

Who are they:

```{r}
spark %>% 
  select(project_id, origin, commit_count, scala_code, gh_stars, dejavu_duplication, gh_created_at, gh_pushed_at, scaladex) %>%
  my_datatable(page_size=10)
```

After filtering we get

```{r stage2 spark overview}
spark <- stage2_projects %>%
  filter(
    str_detect(tolower(project_id), "spark"), 
    scala_code > 1e5
  )

make_stats(
  add_nrow("Final Spark projects (in corpus)", spark),
  add_num("Final Spark code (in corpus)",spark$scala_code),
  add_percent("Final ratio of Spark code (in corpus)", sum(spark$scala_code)/sum(stage2_projects$scala_code))
) %>%
  my_datatable()

```

Who are they:

```{r}
spark %>% 
  select(project_id, origin, commit_count, scala_code, gh_stars, dejavu_duplication, gh_created_at, gh_pushed_at, scaladex) %>%
  my_datatable(page_size=10)
```

### Top projects

The TOP50 by Scala code, commit count and stars.

```{r stage2 top projects}
bind_rows(
  top_n(stage2_projects, 50, scala_code),
  top_n(stage2_projects, 50, commit_count),
  top_n(stage2_projects, 50, gh_stars)
) %>%
  distinct(project_id, .keep_all = TRUE) %>%
  my_datatable(page_size=10)
```
